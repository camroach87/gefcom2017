---
title: "Data cleaning"
author: Cameron Roach
runtime: shiny
output: 
  html_notebook: 
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: false
---

There are several oddities caused by calendar effects that need to be tidied up in the raw data. The main culprits are,

* Daylight savings time (DST)
* Leap years

# Daylight savings

## Electricity readings

When the time shifts forward by an hour the skipped hour has its row filled with zeros. When time moves back by an hour the repeated hour has two hours worth of demand summed together. These issues both need to be corrected for. _This only applies to electricity values and does not apply to temperature readings._


## Temperature readings

When DST comes into effect in March and time skips forward by an hour we do not observe any electricity measurements - the entire row is zero as expected. However, we do see dry bulb and dew point temperature readings. What do these represent?? They do not appear to be duplicated values nor is there any interpolation taking place.

When DST ends we do not observe two hours worth of values being summed together.

Could this be a case of the electricity values being recorded on DST time and weather variables sticking to a non-DST (sequential) time? This would imply that throughout the DST period the electricity and weather variables are out of sync by an hour for the whole period.

## Explanation

Found this in the spreadsheet which explains how the data is in fact treated.

> *Note: For the months of April & October, hourly system load and weather data is averaged to correct for Daylight Savings Time. In April, the hours before and after are averaged to provide a value for the system load and weather for the DST hour.  The LMP data and non-PTF demand are reported as a zero value for the missing hour. In October, an averaged value is used for the double counted hour for system load and weather to correct for the change back to Eastern Standard Time.

# Leap years

Leap years are included in the data. This means that some years have 366 days of data which is going to mess up the DoY variable.

# Solution

* Probably don't want to convert timestamps away from DST. Maybe want to build a separate model for DST and non-DST periods of the year.
* We can ignore the missing DST hour. No values are real/relevant.
* ~~The doulbe counted and averaged hour needs to be split into two hours. We need to impute estimates for the actual demand and weather across the two hours. Need to make sure these imputed values are consistent (average to) the average value that was recorded.~~ Easier to filter out these aggregated values. Not losing much info. Info gained might not even be close to correct.
* In a leap year, Feb 29 will be given the same DoY value as Feb 29 (i.e. 59). All DoY values after 29 Feb will be shifted down by one. This results in DoY values of 1,2,...,365 for leap years which is consistent with non-leap years.


# Code

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

rm(list=ls())

require(dplyr)
require(tidyr)
require(readxl)
require(lubridate)
require(ggplot2)
require(plotly)
require(DT)
require(caret)
require(myhelpr)

source("../R/loadData.R")

load_zones_ma <- c("SEMASS", "WCMASS", "NEMASSBOST")
load_zones <- c("ME", "NH", "VT", "CT", "RI", load_zones_ma)

smd <- load_smd_data(load_zones, root_dir = "./..")
```

This code will be added to a new function `clean_smd_data`.

```{r data_clean}
dst_times <- read.csv("./../data/dst_ts.csv") %>% 
  mutate(dst_start = ymd_hms(dst_start),
         dst_end = ymd_hms(dst_end))

# Remove DST hours
smd <- smd %>% 
  filter(!(ts %in% dst_times$dst_start)) %>% 
  filter(!(ts %in% dst_times$dst_end))

# Shift DoY for leap years. Feb 29 has DoY == 60
smd <- smd %>% 
  mutate(DoY = if_else(leap_year(Year) & DoY >= 60,
                       DoY - 1, DoY))
```


# Check

When we check the counts per DoY we see some weird shapes. We get a spike on day 59 as expected, but it isn't clear why there is a step up to about 335 and then back down to 310.

```{r}
renderPlot({
  smd %>% 
  with(., table(DoY, Zone)) %>% 
  data.frame() %>% 
  mutate(DoY = as.numeric(DoY)) %>% 
  ggplot(aes(x=DoY, y=Freq)) + 
  geom_line() + 
  facet_wrap(~Zone) +
  ggtitle("DoY counts")
})
```

If we facet by year for one of the zones it becomes a lot clearer why this shape occurs. The step is caused by 2003 data starting around Feb and the step down is caused by 2016 ending around October. Hence, it looks like things are behaving. The small dips are caused by filtering out the DST hours.

```{r}
renderPlot({
  smd %>% 
    filter(Zone == "ME") %>% 
    with(., table(DoY, Year)) %>% 
    data.frame() %>% 
    mutate(DoY = as.numeric(DoY)) %>% 
    ggplot(aes(x=DoY, y=Freq)) + 
    geom_line() + 
    facet_wrap(~Year) +
    ggtitle("DoY counts")
})
```


Here is a quick visualisation to check that the demand has been properly tidied up.

```{r, echo = FALSE}
inputPanel(
  selectInput(inputId = "i_load_zone",
              label = "Load zone",
              choices = load_zones),
  numericInput(inputId = "i_year",
              label = "Year",
              value = 2015)
)

renderPlot({
  smd %>% 
    filter(Zone == input$i_load_zone,
           Year == input$i_year) %>% 
    ggplot(aes(x = ts, y = Demand)) + 
    geom_line()
})
```
