---
title: "SMD Boosting Models"
author: "Cameron Roach"
output:
  pdf_document:
    toc: yes
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

rm(list=ls())

require(dplyr)
require(tidyr)
require(readxl)
require(lubridate)
require(ggplot2)
theme_set(theme_bw())
require(plotly)
require(caret)
require(myhelpr)
require(doMC)
require(gefcom2017)

registerDoMC(cores = 11)

# Inputs
load_zones_ma <- c("SEMASS", "WCMASS", "NEMASSBOST")
load_zones <- c("ME", "NH", "VT", "CT", "RI", load_zones_ma)
agg_zones <- c("MASS", "TOTAL")
all_zones <- c(load_zones, agg_zones)

fcst_start_date <- dmy("1/5/2016")
fcst_end_date <- dmy("1/6/2016") - 1

# Load data
smd <- load_smd_data(load_zones, root_dir = ".")
smd <- clean_smd_data(smd, root_dir = ".")

# separate data frames for aggregated zones because may change modelling, 
# e.g., remove average of variables and include all individual ones.
smd_mass <- smd %>% 
  filter(Zone %in% load_zones_ma) %>% 
  group_by(Date, Hour, Holiday, Holiday_flag, ts, Period, Year, Month, DoW,
           DoY, Weekend) %>% 
  summarise(Demand = sum(Demand),
            DryBulb = mean(DryBulb),
            DewPnt = mean(DewPnt),
            DryDewDiff = mean(DryDewDiff)) %>% 
  ungroup() %>% 
  mutate(Zone = "MASS")

smd_total <- smd %>% 
  group_by(Date, Hour, Holiday, Holiday_flag, ts, Period, Year, Month, DoW,
           DoY, Weekend) %>% 
  summarise(Demand = sum(Demand),
            DryBulb = mean(DryBulb),
            DewPnt = mean(DewPnt),
            DryDewDiff = mean(DryDewDiff)) %>% 
  ungroup() %>% 
  mutate(Zone = "TOTAL")

# create lagged predictors
smd <- smd %>%
  group_by(Zone) %>% 
  do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = c(1, 2, 3, 4, 24:27, 48:51))) %>% 
  #do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = 1:48)) %>% 
  ungroup() %>% 
  filter(Year >= 2011)

smd_mass <- smd_mass %>% 
  do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = c(1, 2, 3, 4, 24:27, 48:51))) %>% 
  filter(Year >= 2011)

smd_total <- smd_total %>% 
  do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = c(1, 2, 3, 4, 24:27, 48:51))) %>% 
  filter(Year >= 2011)
```

# TODO

* Think about converting Hour from hour ending to hour beginning. Ensures that the date and hour match up.
* Maybe have 2 models. One for DST months and another for non-daylight savings times? When DST starts on March 8th, the time of the peak shifts back one period (from 19 to 20). So, basically, it looks like demand is behaving as before (peak still occurs at same non-DST time), but because we have shifted to DST it looks like the peak has been shifted back by an hour. This may be affecting the model fit.
* IMPORTANT: Add in day of year or day of season/modelling period to models.


# Introduction

This notebook summarises the selected model and forecasts for the 2017 GEFCom-D competition.

## Data

Since this is for the GEFCom-D track of the competition only given data may be used. Hence, we will be able to use data for

* dates
* hour ending,
* holidays,
* dry bulb temperature,
* dew point temperature,
* demand.

Additionally, several dummy variables have been calculated. These include,

* month of year,
* period of day (1-24, ordered factor),
* season,
* holiday flag (boolean),
* day of week,
* weekend (boolean),
* dry bulb and dew point temperature differences.

~~Period of day should be used in place of Hour in modelling as it is a factor.~~ It appears as though boosting algorithm works fine with Hour (perhaps even better - TODO: check).

Lagged demand predictors will not be used in the model. This is because demand is correlated with day of the week whereas weather conditions are not. Hence, when bootstrapping, the relationship between demand and day of the week will not be preserved due to bootstrap blocks being shifted from there original position.

## Output

We wish to produce quantile forecasts (10, 20, ..., 90) for each day in a future month. These forecasts will be produced for each of the zones and also the two aggregated zones (Massachusetts and total of all).

Since we are not provided forecast weather data for the month we will need to rely on a bootstrapping approach to create a distribution.

# Data

The below plots show some useful summary statistics of the load zone data. A small portion of the time-series data is given for each of the zones. Boxplots show the hourly demand values. 

```{r data_exploration}
smd %>% 
  filter(Year == 2015, Month == "Jan") %>% 
  ggplot(aes(x = ts, y = Demand)) + 
  geom_line() + 
  facet_wrap(~Zone) +
  ggtitle("Demand in January 2015")

smd %>% 
  ggplot(aes(x = Zone, y = Demand)) + 
  geom_boxplot() +
  ggtitle("Boxplots of hourly demand by zone.")
```

## Aggregated zones

Two aggregated zones are to be forecast. SEMASS, WCMASS and NEMASSBOST for the Massachusetts zone (MASS) and the sum of all eight zones is the total zone (TOTAL). As a starting point, each weather variable used in the model will be the average of the same weather variable in the zones that make up the aggregated zone.


```{r}
smd_mass %>% 
  select(Zone, Demand, ts, Year, Month) %>% 
  bind_rows(smd_total %>% 
              select(Zone, Demand, ts, Year, Month)) %>% 
  filter(Year == 2015, Month == "Jan") %>% 
  ggplot(aes(x = ts, y = Demand)) + 
  geom_line() + 
  facet_wrap(~Zone) +
  ggtitle("Demand in January 2015 for aggregated zones")

smd_mass %>% 
  select(Zone, Demand, ts) %>% 
  bind_rows(smd_total %>% 
              select(Zone, Demand, ts)) %>% 
  ggplot(aes(x = Zone, y = Demand)) + 
  geom_boxplot() +
  ggtitle("Boxplots of hourly demand by aggregated zone.")
```




# Modelling

This project will be broken into several parts.

1. Fit a model using 2011-2016 data. This data contains public holiday information. Test several models using time-series cross-validation.
2. Bootstrap weather data (all data 2003-2016). Check for any trends - unlikely over that period. Feed bootstrapped data into model to produce demand traces.
3. Reconcile aggregated and zonal demand forecasts (for each bootstrap sample) using `hts` package.
4. Select peak daily demand and add bootstrapped residuals.
5. Calculate quantiles for each day in month of interest based on bootstrapped results.

## Model training

### Load zones

We use a linearly boosted model for all zones with three days of hourly lagged variables for dry bulb and dew point temperatures. L2-regularization is used to penalise predictors. See the vignettes `boosting-tests.Rmd` and `boosting-with-regularization.Rmd` for a discussion of why this model was chosen.

_Only doing 3-fold CV to speed things up. Have already done model validation in previous docs._

```{r fit_zones}
xgb_ctrl <- trainControl(method = "repeatedcv",
                         number = 3,
                         allowParallel = TRUE)

xgb_grid_linear <- expand.grid(nrounds = 300,
                               alpha = 0,
                               lambda = c(0, exp(0:6)),
                               eta = 0.1)

if (file.exists("./cache/xgb_fit_smd_load_forecasting.RData")) {
  load("./cache/xgb_fit_smd_load_forecasting.RData")
} else {
  xgb_fit <- NULL
  system.time({
    for (iZ in load_zones) {
      cat("Fitting model for zone", iZ, "...\n")
      xgb_fit[[iZ]] <- smd %>% 
        filter(Zone == iZ) %>% 
        # select(Demand, Period, DoY, DoW, starts_with("DryBulb"), starts_with("DewPnt")) %>%
        select(Demand, Hour, DoY, DoW, starts_with("DryBulb"), starts_with("DewPnt")) %>%
        mutate_at(vars(starts_with("DryBulb"), starts_with("DewPnt")),
                  scale, center = FALSE) %>% 
        train(Demand ~ . ,
              data = .,
              method="xgbLinear",
              trControl = xgb_ctrl,
              tuneGrid = xgb_grid_linear,
              nthread = 1)
    }
  })
}
```


```{r performance_zones}
results_df <- NULL
for (iZ in names(xgb_fit)) {
  results_df <- data.frame(Zone = iZ,
                           xgb_fit[[iZ]]$results) %>% 
    bind_rows(results_df, .)
}

results_df %>% 
  ggplot(aes(x = lambda, y = RMSE, colour = Zone)) +
  geom_line() +
  ggtitle("3-fold CV RMSE by zone") +
  scale_x_continuous(trans = "log",
                     labels = function(x) round(x, 0)) +
  facet_wrap(~Zone, scales = "free_y") +
  theme(legend.position = "none")
```


### Aggregated zones

Two aggregated zones need to be forecast. One for the three zones comprising Massachusetts and another for the sum of all eight zones.

```{r fit_agg_zones}
if (!file.exists("./cache/xgb_fit_smd_load_forecasting.RData")) {
  system.time({
    cat("Fitting model for zone MASS...\n")
    xgb_fit[["MASS"]] <- smd_mass %>% 
      #select(Demand, Period, DoY, DoW, starts_with("DryBulb"), starts_with("DewPnt")) %>%
      select(Demand, Hour, DoY, DoW, starts_with("DryBulb"), starts_with("DewPnt")) %>%
      mutate_at(vars(starts_with("DryBulb"), starts_with("DewPnt")),
                scale, center = FALSE) %>% 
      train(Demand ~ . ,
            data = .,
            method="xgbLinear",
            trControl = xgb_ctrl,
            tuneGrid = xgb_grid_linear,
            nthread = 1)
    
    cat("Fitting model for zone TOTAL...\n")
    xgb_fit[["TOTAL"]] <- smd_total %>% 
      #select(Demand, Period, DoY, DoW, starts_with("DryBulb"), starts_with("DewPnt")) %>%
      select(Demand, Hour, DoY, DoW, starts_with("DryBulb"), starts_with("DewPnt")) %>%
      mutate_at(vars(starts_with("DryBulb"), starts_with("DewPnt")),
                scale, center = FALSE) %>% 
      train(Demand ~ . ,
            data = .,
            method="xgbLinear",
            trControl = xgb_ctrl,
            tuneGrid = xgb_grid_linear,
            nthread = 1)
  })
  
  save(xgb_fit, file = "./cache/xgb_fit_smd_load_forecasting.RData")
}
```


```{r performance_agg_zones}
for (iZ in agg_zones) {
  results_df <- data.frame(Zone = iZ,
                           xgb_fit[[iZ]]$results) %>% 
    bind_rows(results_df, .)
}

results_df %>% 
  filter(Zone %in% agg_zones) %>% 
  ggplot(aes(x = lambda, y = RMSE, colour = Zone)) +
  geom_line() +
  ggtitle("3-fold CV RMSE by aggregated zone") +
  scale_x_continuous(trans = "log",
                     labels = function(x) round(x, 0)) +
  facet_wrap(~Zone, scales = "free_y") +
  theme(legend.position = "none")
```


## Residual calcuation

Later residuals will be bootstrapped and added to model predictions. Here we calculate the historical residuals.

```{r calc_residuals}
# Manually specify which variables we want to avoid including predictors 
# in case aggregated zones use different predictors.
pred_df_vars <- c("ts", "Date", "DoY", "Year", "Hour", "Period", "Zone", 
                  "Holiday", "Holiday_flag", "Demand", "Prediction")

# none of this code is nice :(
resid_df <- NULL
for (iZ in load_zones) {
  # remember to scale predictor data again!
  resid_df <- smd %>% 
    filter(Zone == iZ) %>% 
    mutate_at(vars(starts_with("DryBulb"), starts_with("DewPnt")),
              scale, center = FALSE) %>% 
    data.frame(.,
               Prediction = predict(xgb_fit[[iZ]], .)) %>% 
    select(one_of(pred_df_vars)) %>% 
    bind_rows(resid_df)
}

# MASS predictions
resid_df <- smd_mass %>% 
  mutate_at(vars(starts_with("DryBulb"), starts_with("DewPnt")),
              scale, center = FALSE) %>% 
    data.frame(.,
               Prediction = predict(xgb_fit[["MASS"]], .)) %>% 
    select(one_of(pred_df_vars)) %>% 
    bind_rows(resid_df)

# TOTAL predictions
resid_df <- smd_total %>% 
  mutate_at(vars(starts_with("DryBulb"), starts_with("DewPnt")),
              scale, center = FALSE) %>% 
    data.frame(.,
               Prediction = predict(xgb_fit[["TOTAL"]], .)) %>% 
    select(one_of(pred_df_vars)) %>% 
    bind_rows(resid_df)


resid_df <- resid_df %>%
  mutate(Residual = Demand - Prediction,
         Zone = factor(Zone, levels = all_zones, ordered = T))
```

```{r residual_plots}
resid_df %>% 
  ggplot(aes(x = DoY, y = Residual)) +
  geom_point(shape = 21, alpha = 0.15) +
  geom_smooth() +
  facet_wrap(~Zone, scales = "free_y") +
  ggtitle("Residuals across year")

resid_df %>% 
  filter(Year == 2015) %>% 
  ggplot(aes(x = Hour, y = Residual)) +
  geom_point(shape = 21, alpha = 0.15) +
  geom_smooth(se = F) +
  facet_wrap(~Zone, scales = "free_y") +
  ggtitle("Residuals across day")

resid_df %>% 
  filter(Year == 2015) %>% 
  ggplot(aes(x = Demand, y = Residual)) +
  geom_point(shape = 21, alpha = 0.15) +
  geom_smooth(se = F) +
  facet_wrap(~Zone, scales = "free") +
  ggtitle("Residuals and demand")

# resid_df %>% 
#   #filter(Zone %in% sample(all_zones, 3)) %>% 
#   ggplot(aes(x=factor(Hour), y=Residual)) + 
#   geom_boxplot() +
#   facet_wrap(~Zone, scales = "free") +
#   xlab("Hour") +
#   #ggtitle("Hourly boxplots of residuals",
#   #        "For 3 randomly sampled zones")
#   ggtitle("Hourly boxplots of residuals")
```

```{r residual_autocorrelation}
acf_df <- NULL
for(iZ in all_zones) {
  
  tmp <- acf(resid_df %>% 
    filter(Zone == iZ) %>% 
    select(Residual),
    plot = FALSE, lag.max = 24*30)
  
  acf_df <- data.frame(Zone = iZ,
                       acf = tmp$acf,
                       lag = tmp$lag,
                       Zone = iZ) %>% 
    bind_rows(acf_df)
}

ggplot(acf_df, aes(x=lag, y=acf, colour = Zone)) +
  geom_line() +
  #facet_wrap(~Zone) +
  ggtitle("Autocorrelation plots")
```



# Bootstrapping

During the bootstrapping stage care must be taken to preserve correlations present in the historical data. This includes temporal correlations such as,

* daily weather correlations,

and spatial correlations such as,

* demand correlation between zones.

## Weather bootstrapping and predictions

Historical weather data will be bootstrapped in blocks that preserve intra-day weather correlation and seasonal weather correlation. Here we use a similar method to Hyndman and Shu (2010) (see bootstrapping doc for more info). Simulated seasons are then input into the model that was previously fitted for each zone.

```{r bootstrap_weather}
bs_dates <- dbl_block_bs(smd$Date,
                         start_date = fcst_start_date,
                         end_date = fcst_end_date,
                         n_sims = 1000, 
                         avg_block_len = 14,
                         delta_loc = 3,
                         delta_len = 3)

weather_bs <- smd %>% 
    mutate(Date = as.Date(Date)) %>% 
    right_join(bs_dates) %>% 
    mutate(ts = ymd_h(paste(Date_seq, Hour)))

############################################################
### Predictions
### TODO: turn into function. Gets used above as well.
### Inputs: x, scale_vars
### Use: weather_bs %>% filter(Zone==iZ) %>% do(predict_zone(.)) %>% bind_rows(fcst_df)
############################################################

# none of this code is nice :(
fcst_df <- NULL
for (iZ in load_zones) {
  # remember to scale predictor data again!
  fcst_df <- weather_bs %>% 
    filter(Zone == iZ) %>% 
    mutate_at(vars(starts_with("DryBulb"), starts_with("DewPnt")),
              scale, center = FALSE) %>% 
    data.frame(.,
               Prediction = predict(xgb_fit[[iZ]], .)) %>% 
    select(one_of(fcst_df_vars)) %>% 
    bind_rows(fcst_df)
}

# MASS predictions
fcst_df <- weather_bs %>% 
  mutate_at(vars(starts_with("DryBulb"), starts_with("DewPnt")),
              scale, center = FALSE) %>% 
    data.frame(.,
               Prediction = predict(xgb_fit[["MASS"]], .)) %>% 
    select(one_of(fcst_df_vars)) %>% 
    bind_rows(fcst_df)

# TOTAL predictions
fcst_df <- weather_bs %>% 
  mutate_at(vars(starts_with("DryBulb"), starts_with("DewPnt")),
              scale, center = FALSE) %>% 
    data.frame(.,
               Prediction = predict(xgb_fit[["TOTAL"]], .)) %>% 
    select(one_of(fcst_df_vars)) %>% 
    bind_rows(fcst_df)
```



## Residual bootstrapping

In addition to bootstrapping historical weather data to obtain weather scenarios, bootstrapping of the residuals needs to be carried out. These bootstrapped residuals can then be added back to the fitted values to produce the estimated demand values.

We expect to see correlation in demand between zones. To preserve this correlation, a blocked bootstrap approach will be taken, where each block contains all the zones. This way, zones are shifted together thereby preserving there correlations. This has the advantage of allowing us to simply sum all of the residuals within an aggregated zone to obtain its aggregated residual. _See Souhaib "bagging quantile regression"._

Based on the ACF plots produced earlier, it looks as though correlation only really holds for for two to three days in the residuals. We will use a simple block bootstrapping process, again similar to Hyndman and Shu (2010). All of the historical years of data will be broken into blocks of length 3 days and then randomly sampled. The bootstrapped residual series will then be added to the predicted values. This will be carried out for each simulated season.

**TODO: write another bootstrapping function for this. Should be quick. Then just add the bootstrapped residuals to the predicted demand based off simulated weather seasons.**


# Forecast reconciliation

R's `hts` package can be used to reconcile the means of heirarchical forecasts. Since we are forecasting the conditional mean on the bootstrap samples we can use `hts` to reconcile the aggregated means and zone means. Empirical quantiles may then be calculated from these reconciled forecasts for each zone and aggregated zone.


# Quantile calculations

Quantiles may be calculated for each hour of each day by simply calculating the empirical quantiles of the bootstrapped demand traces. R's `quantile` function is all that is needed for this.


# Forecast validation

* Calculate pinabll loss scores for an out of sample month.
* Check pinball loss scores for the month by year.
    + There might have cases when a year is consistently around 90 quantile, another year where demand is consistently around 10th quantile.
    + Is it possible to do some sort of normalisation on demand by year? Can then just scale the quantiles by forecast consumption for the month. Adds extra uncertainty though... if the consumption forecast is wrong everything will be way off.
    + Maybe have lagged variables from previous month or two? could be demand at same time in previous months or a summary statistics like average demand in month. Could fix this and help compensate for if it's a low or high consumption year.

# Output

Performance will be assessed using the pinball loss function.
