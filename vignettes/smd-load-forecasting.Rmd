---
title: "GEFCom 2017 demand forecasting"
author: "Cameron Roach"
output:
  pdf_document:
    includes:
      in_header: mystyles.sty
    toc: yes
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	dev = "png",
	dpi = 150
)

rm(list=ls())

require(dplyr)
require(tidyr)
require(readxl)
require(lubridate)
require(ggplot2)
theme_set(theme_bw())
require(plotly)
require(caret)
require(myhelpr)
require(doMC)
require(gefcom2017)
require(WriteXLS)

registerDoMC(cores = 11)

# Inputs
load_zones_ma <- c("SEMASS", "WCMASS", "NEMASSBOST")
load_zones <- c("ME", "NH", "VT", "CT", "RI", load_zones_ma)
agg_zones <- c("TOTAL", "MASS")
all_zones <- c("TOTAL", "ME", "NH", "VT", "CT", "RI", "MASS", load_zones_ma) # hierarchical order
hts_recon_flag <- FALSE
n_sims <- 1000
fcst_start_date <- dmy("1/2/2016")
fcst_end_date <- dmy("1/3/2016") - 1

# Load data
smd <- load_smd_data(load_zones, root_dir = ".")
smd <- clean_smd_data(smd, root_dir = ".")

# separate data frames for aggregated zones because may change modelling, 
# e.g., remove average of variables and include all individual ones.
smd_mass <- smd %>% 
  filter(Zone %in% load_zones_ma) %>% 
  group_by(Date, Hour, Holiday, Holiday_flag, ts, Period, Year, Month, DoW,
           DoY, Weekend) %>% 
  summarise(Demand = sum(Demand),
            DryBulb = mean(DryBulb),
            DewPnt = mean(DewPnt),
            DryDewDiff = mean(DryDewDiff)) %>% 
  ungroup() %>% 
  mutate(Zone = "MASS")

smd_total <- smd %>% 
  group_by(Date, Hour, Holiday, Holiday_flag, ts, Period, Year, Month, DoW,
           DoY, Weekend) %>% 
  summarise(Demand = sum(Demand),
            DryBulb = mean(DryBulb),
            DewPnt = mean(DewPnt),
            DryDewDiff = mean(DryDewDiff)) %>% 
  ungroup() %>% 
  mutate(Zone = "TOTAL")

# create lagged predictors
smd <- smd %>%
  group_by(Zone) %>% 
  do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = 1:72)) %>% 
  ungroup() %>% 
  filter(Year >= 2009)

smd_mass <- smd_mass %>% 
  do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = 1:72)) %>% 
  filter(Year >= 2009)

smd_total <- smd_total %>% 
  do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = 1:72)) %>% 
  filter(Year >= 2009)
```

# Introduction

This notebook summarises the selected model and forecasts for the 2017 GEFCom-D competition.

## Data

Since this is for the GEFCom-D track of the competition only given data may be used. Hence, we will be able to use data for

* dates
* hour ending,
* holidays,
* dry bulb temperature,
* dew point temperature,
* demand.

Additionally, several dummy variables have been calculated. These include,

* month of year,
* period of day (1-24, ordered factor),
* season,
* holiday flag (boolean),
* day of week,
* weekend (boolean),
* dry bulb and dew point temperature differences.

~~Period of day should be used in place of Hour in modelling as it is a factor.~~ It appears as though boosting algorithm works fine with Hour (perhaps even better - TODO: check).

Lagged demand predictors will not be used in the model. This is because demand is correlated with day of the week whereas weather conditions are not. Hence, when bootstrapping, the relationship between demand and day of the week will not be preserved due to bootstrap blocks being shifted from their original position.

## Output

We wish to produce quantile forecasts (10, 20, ..., 90) for every hour in a given month in the future. These forecasts will be produced for each of the zones and also the two aggregated zones (Massachusetts and total of all).

Since we are not provided forecast weather data for the month we will need to rely on a bootstrapping approach to create a distribution.

# Data

The below plots show some useful summary statistics of the load zone data. A small portion of the time-series data is given for each of the zones. Boxplots show the hourly demand values. 

```{r data_exploration}
smd %>% 
  filter(Year == 2015, Month == "Jan") %>% 
  ggplot(aes(x = ts, y = Demand)) + 
  geom_line() + 
  facet_wrap(~Zone) +
  ggtitle("Demand in January 2015")

smd %>% 
  mutate(Year = factor(Year)) %>% 
  ggplot(aes(x = Year, y = Demand)) + 
  geom_boxplot() +
  facet_wrap(~Zone, scales = "free_y") +
  ggtitle("Boxplots of hourly demand by zone.")
```

## Aggregated zones

Two aggregated zones are to be forecast. SEMASS, WCMASS and NEMASSBOST for the Massachusetts zone (MASS) and the sum of all eight zones is the total zone (TOTAL). As a starting point, each weather variable used in the model will be the average of the same weather variable in the zones that make up the aggregated zone.


```{r}
smd_mass %>% 
  select(Zone, Demand, ts, Year, Month) %>% 
  bind_rows(smd_total %>% 
              select(Zone, Demand, ts, Year, Month)) %>% 
  filter(Year == 2015, Month == "Jan") %>% 
  ggplot(aes(x = ts, y = Demand)) + 
  geom_line() + 
  facet_wrap(~Zone) +
  ggtitle("Demand in January 2015 for aggregated zones")

smd_mass %>% 
  select(Zone, Demand, ts, Year) %>% 
  bind_rows(smd_total %>% 
              select(Zone, Demand, ts, Year)) %>% 
  mutate(Year = factor(Year)) %>% 
  ggplot(aes(x = Year, y = Demand)) + 
  geom_boxplot() +
  facet_wrap(~Zone, scales = "free_y") +
  ggtitle("Boxplots of hourly demand by aggregated zone.")
```




# Modelling

This project will be broken into several parts.

1. Fit a model using 2011-2016 data. This data contains public holiday information. Test several models using time-series cross-validation.
2. Bootstrap weather data (all data 2003-2016). Check for any trends - unlikely over that period. Feed bootstrapped data into model to produce demand traces.
3. Reconcile aggregated and zonal demand forecasts (for each bootstrap sample).
4. Select peak daily demand and add bootstrapped residuals.
5. Calculate quantiles for each day in month of interest based on bootstrapped results.

## Model training

### Load zones

We use a linearly boosted model for all zones with three days of hourly lagged variables for dry bulb and dew point temperatures. L2-regularization is used to penalise predictors. See the vignettes `boosting-tests.Rmd` and `boosting-with-regularization.Rmd` for a discussion of why this model was chosen.

```{r fit_zones}
xgb_ctrl <- trainControl(method = "repeatedcv",
                         number = 5,
                         allowParallel = TRUE)

xgb_grid_linear <- expand.grid(nrounds = 300,
                               alpha = 0,
                               lambda = c(0, exp(0:4)),
                               eta = 0.1)

if (file.exists("./cache/smd_load_forecasting.RData")) {
  load("./cache/smd_load_forecasting.RData")
} else {
  xgb_fit <- NULL
  system.time({
    for (iZ in load_zones) {
      cat("Fitting model for zone", iZ, "...\n")
      xgb_fit[[iZ]] <- smd %>% 
        filter(Zone == iZ) %>% 
        select(Demand, Hour, DoY, DoW, Holiday_flag,
               starts_with("DryBulb"), starts_with("DewPnt")) %>%
        train(Demand ~ . ,
              data = .,
              method="xgbLinear",
              trControl = xgb_ctrl,
              tuneGrid = xgb_grid_linear,
              nthread = 1)
    }
  })
}
```


### Aggregated zones

Two aggregated zones need to be forecast. One for the three zones comprising Massachusetts and another for the sum of all eight zones.

```{r fit_agg_zones}
if (!file.exists("./cache/smd_load_forecasting.RData")) {
  system.time({
    cat("Fitting model for zone MASS...\n")
    xgb_fit[["MASS"]] <- smd_mass %>% 
      select(Demand, Hour, DoY, DoW, Holiday_flag, 
             starts_with("DryBulb"), starts_with("DewPnt")) %>%
      train(Demand ~ . ,
            data = .,
            method="xgbLinear",
            trControl = xgb_ctrl,
            tuneGrid = xgb_grid_linear,
            nthread = 1)

    cat("Fitting model for zone TOTAL...\n")
    xgb_fit[["TOTAL"]] <- smd_total %>% 
      select(Demand, Hour, DoY, DoW, Holiday_flag, 
             starts_with("DryBulb"), starts_with("DewPnt")) %>%
      train(Demand ~ . ,
            data = .,
            method="xgbLinear",
            trControl = xgb_ctrl,
            tuneGrid = xgb_grid_linear,
            nthread = 1)
  })
  
  save(xgb_fit, file = "./cache/smd_load_forecasting.RData")
}
```


### Performance

```{r performance_zones}
results_df <- NULL
for (iZ in names(xgb_fit)) {
  results_df <- data.frame(Zone = iZ,
                           xgb_fit[[iZ]]$results) %>% 
    bind_rows(results_df, .)
}

results_df %>% 
  mutate(Zone = factor(Zone, levels = all_zones, ordered = T)) %>% # order subplots
  ggplot(aes(x = lambda, y = RMSE, colour = Zone)) +
  geom_line() +
  ggtitle("5-fold CV RMSE by zone") +
  scale_x_continuous(trans = "log",
                     labels = function(x) round(x, 0)) +
  facet_wrap(~Zone, scales = "free_y") +
  theme(legend.position = "none")
```


## Residual calcuation

Later residuals will be bootstrapped and added to model predictions. Here we calculate the historical residuals.

```{r calc_residuals}
# Manually specify which variables we want to avoid including predictors 
# in case aggregated zones use different predictors.
resid_df_vars <- c("ts", "Date", "DoY", "DoW", "Year", "Hour", "Period",
                   "Zone", "Holiday", "Holiday_flag", "Demand", "Prediction")

# none of this code is nice :(
resid_df <- NULL
for (iZ in load_zones) {
  resid_df <- smd %>% 
    filter(Zone == iZ) %>% 
    data.frame(.,
               Prediction = predict(xgb_fit[[iZ]], .)) %>% 
    select(one_of(resid_df_vars)) %>% 
    bind_rows(resid_df)
}

# MASS predictions
resid_df <- smd_mass %>% 
  data.frame(.,
             Prediction = predict(xgb_fit[["MASS"]], .)) %>% 
  select(one_of(resid_df_vars)) %>% 
  bind_rows(resid_df)

# TOTAL predictions
resid_df <- smd_total %>% 
  data.frame(.,
             Prediction = predict(xgb_fit[["TOTAL"]], .)) %>% 
  select(one_of(resid_df_vars)) %>% 
  bind_rows(resid_df)


resid_df <- resid_df %>%
  mutate(Residual = Demand - Prediction,
         Zone = factor(Zone, levels = all_zones, ordered = T)) # order subplots
```

```{r residual_plots}
resid_df %>% 
  ggplot(aes(x = DoY, y = Residual)) +
  geom_point(shape = 21, alpha = 0.15) +
  geom_smooth() +
  facet_wrap(~Zone, scales = "free_y") +
  ggtitle("Residuals across year")

resid_df %>% 
  filter(Year == 2015) %>% 
  ggplot(aes(x = Hour, y = Residual)) +
  geom_point(shape = 21, alpha = 0.15) +
  geom_smooth(se = F) +
  facet_wrap(~Zone, scales = "free_y") +
  ggtitle("Residuals across day")

resid_df %>% 
  filter(Year == 2015) %>% 
  ggplot(aes(x = Demand, y = Residual)) +
  geom_point(shape = 21, alpha = 0.15) +
  geom_smooth(se = F) +
  facet_wrap(~Zone, scales = "free") +
  ggtitle("Residuals and demand")
```

```{r residual_autocorrelation}
acf_df <- NULL
for(iZ in all_zones) {
  
  tmp <- acf(resid_df %>% 
    filter(Zone == iZ) %>% 
    select(Residual),
    plot = FALSE, lag.max = 24*30)
  
  acf_df <- data.frame(Zone = iZ,
                       acf = tmp$acf,
                       lag = tmp$lag,
                       Zone = iZ) %>% 
    bind_rows(acf_df)
}

ggplot(acf_df, aes(x=lag, y=acf, colour = Zone)) +
  geom_line() +
  #facet_wrap(~Zone) +
  ggtitle("Autocorrelation plots")
```



# Bootstrapping

During the bootstrapping stage care must be taken to preserve correlations present in the historical data. This includes temporal correlations such as,

* daily weather correlations,

and spatial correlations such as,

* demand correlation between zones.

## Weather bootstrapping and predictions

Historical weather data will be bootstrapped in blocks that preserve intra-day weather correlation and seasonal weather correlation. Here we use a similar method to Hyndman and Shu (2010) (see bootstrapping doc for more info). Simulated seasons are then input into the model that was previously fitted for each zone.

**TODO: Have removed scaling from variables as scaling the bootstrapped data causes issues. It means we are scaling the training data by a different amount to the data used for bootstrap predictions. Perhaps there is a way to do scaling (think about it), but ignoring for the moment because it causes issues.**

```{r bootstrap_weather}
bs_dates <- dbl_block_bs(smd$Date,
                         start_date = fcst_start_date,
                         end_date = fcst_end_date,
                         n_sims = n_sims, 
                         avg_block_len = 14,
                         delta_loc = 3,
                         delta_len = 3)

weather_bs <- smd %>% 
  mutate(Date = as.Date(Date)) %>% 
  right_join(bs_dates) %>% 
  mutate(ts = ymd_h(paste(Date_seq, Hour))) %>% 
  select(-Date) %>% 
  rename(Date = Date_seq) %>% 
  get_calendar_vars(.) %>% # need to be recalculated based on sequential dates
  clean_smd_data(.)

weather_bs_mass <- smd_mass %>% 
  mutate(Date = as.Date(Date)) %>% 
  right_join(bs_dates) %>% 
  mutate(ts = ymd_h(paste(Date_seq, Hour))) %>% 
  select(-Date) %>% 
  rename(Date = Date_seq) %>% 
  get_calendar_vars(.) %>%  # need to be recalculated based on sequential dates
  clean_smd_data(.)

weather_bs_total <- smd_total %>% 
  mutate(Date = as.Date(Date)) %>% 
  right_join(bs_dates) %>% 
  mutate(ts = ymd_h(paste(Date_seq, Hour))) %>% 
  select(-Date) %>% 
  rename(Date = Date_seq) %>% 
  get_calendar_vars(.) %>% # need to be recalculated based on sequential dates
  clean_smd_data(.)







# TODO: Might want to turn below into a function since a similar version is used
# above as well. Not interested in demand because underlying weather/calendar
# variables have changed after bootstrapping

fcst_bs_df_vars <- c("Simulation", "ts", "Date", "Hour", "DoY", "DoW", "Year", 
                     "Month", "Period", "Zone", "Holiday", "Holiday_flag",
                     "Weekend", "Prediction")

fcst_bs_df <- NULL
for (iZ in load_zones) {
  # remember to scale predictor data again!
  fcst_bs_df <- weather_bs %>% 
    filter(Zone == iZ) %>% 
    data.frame(.,
               Prediction = predict(xgb_fit[[iZ]], .)) %>% 
    select(one_of(fcst_bs_df_vars)) %>% 
    bind_rows(fcst_bs_df)
}

# MASS predictions
fcst_bs_df <- weather_bs_mass %>% 
  data.frame(.,
             Prediction = predict(xgb_fit[["MASS"]], .)) %>% 
  select(one_of(fcst_bs_df_vars)) %>% 
  bind_rows(fcst_bs_df)

# TOTAL predictions
fcst_bs_df <- weather_bs_total %>% 
  data.frame(.,
             Prediction = predict(xgb_fit[["TOTAL"]], .)) %>% 
  select(one_of(fcst_bs_df_vars)) %>% 
  bind_rows(fcst_bs_df)
```


```{r plot_fitted_values}
p_sim <- sample(unique(fcst_bs_df$Simulation), 1)
fcst_bs_df %>%
  filter(Simulation == p_sim) %>% 
  ggplot(aes(x=ts, y=Prediction, colour = Zone)) +
  geom_line() +
  ggtitle("Predicted demand for forecast period.",
          paste("Simulation", p_sim))
```



## Residual bootstrapping

In addition to bootstrapping historical weather data to obtain weather scenarios, bootstrapping of the residuals needs to be carried out. These bootstrapped residuals can then be added back to the fitted values to produce the estimated demand values.

We expect to see correlation in demand between zones. To preserve this correlation, a blocked bootstrap approach will be taken, where each block contains all the zones. This way, zones are shifted together thereby preserving there correlations. This has the advantage of allowing us to simply sum all of the residuals within an aggregated zone to obtain its aggregated residual. _See Souhaib "bagging quantile regression"._

Based on the ACF plots produced earlier, it looks as though correlation only really holds for for two to three days in the residuals. We will use a simple block bootstrapping process, again similar to Hyndman and Shu (2010). All of the historical years of data will be broken into blocks of length 3 days and then randomly sampled. The bootstrapped residual series will then be added to the predicted values. This will be carried out for each simulated season.

The hours when DST commence and end have been filtered from the data. This means some days will only have 23 residuals avaialable. These days should be filtered. We could simulate the residual for this hour, but it's probably cleaner just removing this day. It does create a break in the residual series but I doubt the impact will be significant. Effectively, it's just another block break point.

There may also be a block at the end of the time series that is not the same length as the others due. This block is omitted.

```{r bootstrap_residuals}
fcst_bs_df <- fcst_bs_df %>% 
  group_by(Zone, Simulation) %>% 
  do(resid_block_bs(resid_df, ., block_length = 4)) %>% 
  ungroup()

fcst_bs_df %>% 
  mutate(Zone = factor(Zone, levels = all_zones, ordered = T)) %>% # plot order
  ggplot(aes(x = DoY, y = Residual)) +
  geom_point(shape = 21, alpha = 0.15) +
  geom_smooth() +
  facet_wrap(~Zone, scales = "free_y") +
  ggtitle("Simulated residuals across forecast period")

fcst_bs_df %>% 
  mutate(Zone = factor(Zone, levels = all_zones, ordered = T)) %>% # plot order
  ggplot(aes(x = Hour, y = Residual)) +
  geom_point(shape = 21, alpha = 0.15) +
  geom_smooth(se = F) +
  facet_wrap(~Zone, scales = "free_y") +
  ggtitle("Simulated residuals across day")
```


## Combining predictions and residuals

Given our predictions and simulated residuals we can add them to obtain the final simulated demand forecasts.

**TODO: Check performance when no residuals used (set fcst_bs_df$Residual = 0)**


```{r combine_pred_resid}
fcst_bs_df <- fcst_bs_df %>% 
  mutate(Prediction_adj = Prediction + Residual)
```

```{r plot_combine_pred_resid, fig.height=13, fig.width = 9}
fcst_bs_df %>%
  filter(Simulation == p_sim) %>% 
  select(ts, Prediction, Prediction_adj, Zone) %>% 
  gather(var, val, -c(ts, Zone)) %>% 
  ggplot(aes(x=ts, y=val, colour = Zone, linetype = var)) +
  geom_line() +
  scale_linetype_manual(values = c("Prediction_adj" = "solid",
                                   "Prediction" = "dashed")) +
  #facet_wrap(~Zone, scales = "free_y", ncol = 2) +
  ggtitle("Predicted demand and adjusted demand for forecast period.",
          paste("Simulation", p_sim))
```



# Hierarchical forecast reconciliation

Since we are dealing with hierarchical forecasts we need to reconcile the bottom level and aggregated time-series. Empirical quantiles may then be calculated from these reconciled forecasts for each zone and aggregated zone.

We are dealing with the hierarchical structure as shown below.

$$
\begin{tikzpicture}[sibling distance=6em,
  font=\sffamily\small,
  every node/.style = {shape=rectangle,
    draw, align=center,
    top color=white, bottom color=blue!20}]
  \node {TOTAL}
    child { node {ME} }
    child { node {NH} }
    child { node {VT} }
    child { node {CT} }
    child { node {RI} }
    child { node {MASS} 
      child { node {SEMASS} }
      child { node {WCMASS} }
      child { node {NEMASSBOST} } };
\end{tikzpicture}
$$

Which can be represented in matrix notation using the summing matrix $S$,

$$
\begin{bmatrix}
  y_{TOTAL,t} \\
  y_{ME,t} \\
  y_{NH,t} \\
  y_{VT,t} \\
  y_{CT,t} \\
  y_{RI,t} \\
  y_{MASS,t} \\
  y_{SEMASS,t} \\
  y_{WCMASS,t} \\
  y_{NEMASSBOST,t} \\
\end{bmatrix} =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
    0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
  \end{bmatrix}
  \begin{bmatrix}
    y_{ME,t} \\
    y_{NH,t} \\
    y_{VT,t} \\
    y_{CT,t} \\
    y_{RI,t} \\
    y_{SEMASS,t} \\
    y_{WCMASS,t} \\
    y_{NEMASSBOST,t} \\
  \end{bmatrix} \\
\mathbf{y}_t = \mathbf{S}\mathbf{y}_{8,t}
$$

For optimal forecast reconciliation the adjust forecasts $\tilde{\mathbf{y}}$ at forecast time $t$ can be calculated with,

$$
\tilde{\mathbf{y}}_t = \mathbf{S}(\mathbf{S'S})^{-1}\mathbf{S'}\hat{\mathbf{y}}_t
$$
where $\hat{\mathbf{y}}_t$ are the original forecasts for each zone at time $t$.

Here's the code.

```{r hierarchical_reconciliation}
S <- matrix(ncol = 8, nrow = 10, byrow = TRUE,
            c(1,1,1,1,1,1,1,1,
              1,0,0,0,0,0,0,0,
              0,1,0,0,0,0,0,0,
              0,0,1,0,0,0,0,0,
              0,0,0,1,0,0,0,0,
              0,0,0,0,1,0,0,0,
              0,0,0,0,0,1,1,1,
              0,0,0,0,0,1,0,0,
              0,0,0,0,0,0,1,0,
              0,0,0,0,0,0,0,1))

h_rec <- function(x, S) {
  S %*% solve(t(S) %*% S) %*% t(S) %*% x
}

hts_raw <- fcst_bs_df %>% 
  select(Simulation, Zone, ts, Date, Hour, Prediction_adj) %>% 
  spread(Zone, Prediction_adj) %>% 
  select(Simulation, ts, Date, Hour, TOTAL, ME, NH, VT, CT, RI, MASS,
         SEMASS, WCMASS, NEMASSBOST)

cat("HTS reconciliation flag set to:", hts_recon_flag, "...\n")

if (hts_recon_flag == TRUE) {
  hts_rec <- t(apply(as.matrix(hts_raw[,-c(1:4)]), 1, h_rec, S))
  colnames(hts_rec) <- c("TOTAL", "ME", "NH", "VT", "CT", "RI", "MASS",
                         "SEMASS", "WCMASS", "NEMASSBOST")
  
  # convert back to a dataframe and correct structure
  hts_rec <- bind_cols(hts_raw[,c(1:4)], as.data.frame(hts_rec)) %>% 
    gather(Zone, Prediction_rec, -c(Simulation, ts, Date, Hour))
  hts_raw <- hts_raw %>% 
    gather(Zone, Prediction_adj, -c(Simulation, ts, Date, Hour))
} else {
  # no adjustment - just set reconciled data frame to raw
  hts_raw <- hts_raw %>% 
    gather(Zone, Prediction_adj, -c(Simulation, ts, Date, Hour))
  hts_rec <- hts_raw %>% 
    mutate(Prediction_rec = Prediction_adj)
}
```

We can see a comparison of the reconciled and unreconciled forecasts in the plot below. The first day in the forecast horizon is shown.

```{r reconciliation_plot}
p_date <- unique(hts_raw$Date)[1:3]
ggplot() +
  geom_line(data = hts_raw %>% 
              filter(Simulation == p_sim,
                     Date %in% p_date) %>% 
              mutate(Zone = factor(Zone, levels = all_zones, ordered = TRUE)),
            aes(x = ts, y = Prediction_adj, colour = "Unreconciled")) +
  geom_line(data = hts_rec %>% 
              filter(Simulation == p_sim,
                     Date %in% p_date) %>% 
              mutate(Zone = factor(Zone, levels = all_zones, ordered = TRUE)),
            aes(x = ts, y = Prediction_rec, colour = "Reconciled")) +
  facet_wrap(~Zone, scales = "free") +
  ylab("Demand") +
  scale_colour_manual(name = NULL,
                      values = c(#"Actual" = "red",
                                 "Reconciled" = "black",
                                 "Unreconciled" = "darkgrey")) +
  ggtitle("Hierarchical forecast reconcilation",
          "Optimal combination approach")
```



# Quantile calculations

Quantiles may be calculated for each hour of each day by simply calculating the empirical quantiles of the bootstrapped demand traces. R's `quantile` function is all that is needed for this.

```{r calc_quantiles}
quantile_fcst <- hts_rec %>% 
  group_by(Zone, ts, Date, Hour) %>% 
  summarise(Q10 = quantile(Prediction_rec, 0.1),
            Q20 = quantile(Prediction_rec, 0.2),
            Q30 = quantile(Prediction_rec, 0.3),
            Q40 = quantile(Prediction_rec, 0.4),
            Q50 = quantile(Prediction_rec, 0.5),
            Q60 = quantile(Prediction_rec, 0.6),
            Q70 = quantile(Prediction_rec, 0.7),
            Q80 = quantile(Prediction_rec, 0.8),
            Q90 = quantile(Prediction_rec, 0.9)) %>% 
  ungroup()
```


# Forecast validation

**TODO: This section should be moved to another document as we will not have actuals for the real forecast period.**

The below plots show where the actual values fall on the quantile forecasts. We show the actuals during the forecast period and where they fall on the quantile forecasts. _Remember: can't use actuals from previous years as day of week effects/holidays may be different._

```{r hist_and_quantile_plots, fig.height = 13, fig.width = 9}
for (iZ in all_zones) {
  historical_plot_data <- resid_df %>% 
    filter(Zone == iZ,
           date(ts) >= fcst_start_date,
           date(ts) <= fcst_end_date)
  
  p <- ggplot() +
    geom_line(data = quantile_fcst %>% 
                filter(Zone == iZ) %>% 
                select(ts, starts_with("Q")) %>% 
                gather(Quantile, Demand, -ts) %>% 
                mutate(Week = week(ts)),
              aes(x = ts, y = Demand, colour = Quantile)) +
    geom_point(data = historical_plot_data %>% 
                 mutate(Week = week(ts)),
               aes(x = ts, y = Demand),
               shape = 21) +
    facet_wrap(~Week, scales = "free", ncol = 1) +
    ggtitle("Historical demand and quantile forecasts.",
            paste("Zone:", iZ)) +
    theme(strip.background = element_blank(),
          strip.text.x = element_blank())
  print(p)
}
```


* Calculate pinabll loss scores for an out of sample month.
* Check pinball loss scores for the month by year.
    + There might have cases when a year is consistently around 90 quantile, another year where demand is consistently around 10th quantile.
    + Is it possible to do some sort of normalisation on demand by year? Can then just scale the quantiles by forecast consumption for the month. Adds extra uncertainty though... if the consumption forecast is wrong everything will be way off.
    + Maybe have lagged variables from previous month or two? could be demand at same time in previous months or a summary statistics like average demand in month. Could fix this and help compensate for if it's a low or high consumption year.

# Output

Forecasts are output to excel in the required format.

```{r excel_output}
sheet_order <- c("CT", "ME", "NEMASSBOST", "NH", "RI", "SEMASS", "VT",
                 "WCMASS", "MASS", "TOTAL")

excel_output <- NULL
for (iS in sheet_order) {
  excel_output[[iS]] <- quantile_fcst %>%
    filter(Zone == iS) %>%
    select(Date, Hour, Q10, Q20, Q30, Q40, Q50, Q60, Q70, Q80, Q90) %>% 
    mutate(Date = format(Date, "%e/%m/%Y"))
}

dir.create("./output", F, T)
WriteXLS(excel_output,
         ExcelFileName = "./output/D3-These boosts are made forecasting.xls")
```

