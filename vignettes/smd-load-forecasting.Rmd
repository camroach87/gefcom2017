---
title: "SMD Boosting Models"
author: Cameron Roach
output: 
  html_notebook: 
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

rm(list=ls())

require(dplyr)
require(tidyr)
require(readxl)
require(lubridate)
require(ggplot2)
require(plotly)
require(caret)
require(myhelpr)
require(doMC)
require(gefcom2017)

registerDoMC(cores = 3)

load_zones_ma <- c("SEMASS", "WCMASS", "NEMASSBOST")
load_zones <- c("ME", "NH", "VT", "CT", "RI", load_zones_ma)

smd <- load_smd_data(load_zones, root_dir = "./..")
smd <- clean_smd_data(smd, root_dir = "./..")

smd <- smd %>%
  group_by(Zone) %>% 
  do(get_lagged_vars(.)) %>% 
  ungroup() %>% 
  filter(Year >= 2012)
```

# TODO

* Think about converting Hour from hour ending to hour beginning. Ensures that the date and hour match up.
* Maybe have 2 models. One for DST months and another for non-daylight savings times? When DST starts on March 8th, the time of the peak shifts back one period (from 19 to 20). So, basically, it looks like demand is behaving as before (peak still occurs at same non-DST time), but because we have shifted to DST it looks like the peak has been shifted back by an hour. This may be affecting the model fit.


# Introduction

This notebook summarises the selected model and forecasts for the 2017 GEFCom-D competition.

## Data

Since this is for the GEFCom-D track of the competition only given data may be used. Hence, we will be able to use data for

* dates
* hour ending,
* holidays,
* dry bulb temperature,
* dew point temperature,
* demand.

Additionally, several dummy variables have been calculated. These include,

* month of year,
* period of day (1-24, ordered factor),
* season,
* holiday flag (boolean),
* day of week,
* weekend (boolean),
* dry bulb and dew point temperature differences.

Period of day should be used in place of Hour in modelling as it is a factor.

## Output

We wish to produce quantile forecasts (10, 20, ..., 90) for each day in a future month. These forecasts will be produced for each of the zones and also the two aggregated zones (Massachusetts and total of all).

Since we are not provided forecast weather data for the month we will need to rely on a bootstrapping approach to create a distribution.

# Modelling

This project will be broken into several parts.

1. Fit a model using 2011-2016 data. This data contains public holiday information. Test several models using time-series cross-validation.
2. Bootstrap weather data (all data 2003-2016). Check for any trends - unlikely over that period. Feed bootstrapped data into model to produce demand traces.
3. Reconcile aggregated and zonal demand forecasts (for each bootstrap sample) using `hts` package.
4. Select peak daily demand and add bootstrapped residuals.
5. Calculate quantiles for each day in month of interest based on bootstrapped results.

## Model training

### Load zones

We use a linearly boosted model for all zones with three days of hourly lagged variables for dry bulb and dew point temperatures. L2-regularization is used to penalise predictors. See the vignettes `boosting-tests.Rmd` and `boosting-with-regularization.Rmd` for a discussion of why this model was chosen.

_Only doing 3-fold CV to speed things up. Have already done model validation in previous docs._

```{r}
xgb_ctrl <- trainControl(method = "repeatedcv",
                         number = 3,
                         allowParallel = TRUE)

xgb_grid_linear <- expand.grid(nrounds = 300,
                               alpha = 0,
                               lambda = 100,
                               eta = 0.1)

xgb_fit <- NULL
system.time({
  for (iZ in load_zones) {
    cat("Fitting model for zone", iZ, "...\n")
    xgb_fit[[iZ]] <- smd %>% 
      filter(Zone == iZ) %>% 
      select(Demand, Period, starts_with("DryBulb"), starts_with("DewPnt")) %>% 
      mutate_at(vars(starts_with("DryBulb"), starts_with("DewPnt")),
                scale, center = FALSE) %>% 
      train(Demand ~ . ,
            data = .,
            method="xgbLinear",
            trControl = xgb_ctrl,
            tuneGrid = xgb_grid_linear,
            nthread = 1)
  }
})
```

### Aggregated zones

Two aggregated zones need to be forecast. One for the three zones comprising Massachusetts and another for the sum of all eight zones.

**TODO**


# Bootstrapping

During the bootstrapping stage care must be taken to preserve correlations present in the historical data. This includes temporal correlations such as,

* daily weather correlations,

and spatial correlations such as,

* demand correlation between zones.

## Weather bootstrapping

Historical weather data will be bootstrapped in blocks that preserve intra-day weather correlation.



## Residual bootstrapping

In addition to bootstrapping historical weather data to obtain weather scenarios, bootstrapping of the residuals needs to be undertaken. These bootstrapped residuals can then be added back to the fitted values to produce the estimated demand values.

We expect to see correlation in demand between zones. To preserve this correlation, a blocked bootstrap approach will be taken, where each block contains all the zones. This way, zones are shifted together thereby preserving there correlations. This has the advantage of allowing us to simply sum all of the residuals within an aggregated zone to obtain its aggregated residual. _See Souhaib "bagging quantile regression"._

## Aggregated forecast reconciliation

R's `hts` package can be used to reconcile the means of heirarchical forecasts. Since we are forecasting the conditional mean on the bootstrap samples we can use `hts` to reconcile the aggregated means and zone means. Empirical quantiles may then be calculated from these reconciled forecasts for each zone and aggregated zone.

# Quantile calculations

Quantiles may be calculated for each hour of each day by simply calculating the empirical quantiles of the bootstrapped demand traces. R's `quantile` function is all that is needed for this.

# Output

Performance will be assessed using the pinball loss function.