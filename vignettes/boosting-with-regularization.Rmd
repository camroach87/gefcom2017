---
title: "Boosting with regularization"
author: "Cameron Roach"
output:
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	dev = "png",
	dpi = 150
)

rm(list=ls())

require(dplyr)
require(tidyr)
require(readxl)
require(lubridate)
require(ggplot2)
theme_set(theme_bw())
require(plotly)
require(caret)
require(myhelpr)
require(doMC)
require(gefcom2017)

registerDoMC(cores = 11)

# Inputs
load_zones_ma <- c("SEMASS", "WCMASS", "NEMASSBOST")
load_zones <- c("ME", "NH", "VT", "CT", "RI", load_zones_ma)
agg_zones <- c("MASS", "TOTAL")
all_zones <- c(load_zones, agg_zones)
n_sims <- 1000
fcst_start_date <- dmy("1/2/2017")
fcst_end_date <- dmy("1/3/2017") - 1

# Load data
smd <- load_smd_data(load_zones, root_dir = ".")
smd <- clean_smd_data(smd, root_dir = ".")

# create lagged predictors
smd <- smd %>%
  group_by(Zone) %>% 
  do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = 1:72)) %>% 
  ungroup()

# training/test data sets
smd_test <- filter(smd, Year > 2015)
smd <- filter(smd,
              Year >= 2011,
              Year <= 2015)
```

# Introduction

Here I wish to test the impact of a L1 and L2 regularization on model accuracy when using boosting.

Electricity demand in the ME load zone will be used. Only working days will be considered (weekends and holidays are filtered). Dry bulb temperature and lags will be used as a predictors. Models will be assessed against each other using RMSE.

# Boosting with manual variable selection

This serves as a baseline model to test if regularization performs better than manual variable selection. These variables have been manually selected based on the analysis in `boosting-tests.Rmd`.

```{r fit_man_variables}
xgb_ctrl <- trainControl(method = "repeatedcv",
                         number = 5,
                         allowParallel = TRUE)

xgb_grid_linear <- expand.grid(nrounds = 300,
                               alpha = 0,
                               lambda = 0,
                               eta = 0.1)

if (file.exists("./cache/boosting_with_regularization_models.RData")) {
  load("./cache/boosting_with_regularization_models.RData")
} else {
  xgb_fit <- NULL
  system.time({
    for (iZ in load_zones) {
      # Based on xgbLinear_lag5 model
      cat("Fitting manual variable selection model for zone", iZ, "...\n")
      
      xgb_fit[[iZ]][["Manual"]] <- smd %>% 
        filter(Zone == iZ) %>%
        train(Demand ~ DryBulb + DryBulb_lag1 + DryBulb_lag2 + DryBulb_lag3 + 
                DryBulb_lag4 + DryBulb_lag5 + DryBulb_lag6 + DryBulb_lag24 + 
                DryBulb_lag48 + DryBulb_lag72 + Hour + DoY + DoW + 
                Holiday_flag,
              data = .,
              method="xgbLinear",
              trControl = xgb_ctrl,
              tuneGrid = xgb_grid_linear,
              nthread = 1)
    }
  })
}
```


# Boosting with L1-regularization

We will use all predictors and test a numer of weights for L1 regularization. All predictors (temperatures and lagged temperatures) have been scaled except for period of day, which is treated as a categorical variable.

```{r fit_boost_l1_reg}
xgb_grid_linear <- expand.grid(nrounds = 300,
                               alpha = c(0, exp(0:7)),
                               lambda = 0,
                               eta = 0.1)

if (!file.exists("./cache/boosting_with_regularization_models.RData")) {
  system.time({
    for (iZ in load_zones) {
      cat("Fitting L1-regularization model for zone", iZ, "...\n")
      
      xgb_fit[[iZ]][["L1-regularization"]] <- smd %>% 
        filter(Zone == iZ) %>% 
        select(Demand, Hour, DoY, DoW, Holiday_flag, starts_with("DryBulb"), 
               starts_with("DewPnt")) %>%
        train(Demand ~ . ,
              data = .,
              method="xgbLinear",
              trControl = xgb_ctrl,
              tuneGrid = xgb_grid_linear,
              nthread = 1)
    }
  })
}
```

## L1-regularization results

We see lowest RMSE's when `alpha` is set to 1000.

```{r model_performance_l1}
# xgb_l1_fit
# 
# ggplot(xgb_l1_fit) +
#   scale_x_log10() +
#   ggtitle("L1 model performance")
```



# Boosting with L2-regularization

We will use all predictors and test a numer of weights for L2 regularization. All predictors (temperatures and lagged temperatures) have been scaled except for period of day, which is treated as a categorical variable.

```{r fit_boost_l2_reg}
xgb_grid_linear <- expand.grid(nrounds = 300,
                               alpha = 0,
                               lambda = c(0, exp(0:7)),
                               eta = 0.1)

if (!file.exists("./cache/boosting_with_regularization_models.RData")) {
  system.time({
    for (iZ in load_zones) {
      cat("Fitting L2-regularization model for zone", iZ, "...\n")
      
      xgb_fit[[iZ]][["L2-regularization"]] <- smd %>% 
        filter(Zone == iZ) %>% 
        select(Demand, Hour, DoY, DoW, Holiday_flag, starts_with("DryBulb"),
               starts_with("DewPnt")) %>%
        train(Demand ~ . ,
              data = .,
              method="xgbLinear",
              trControl = xgb_ctrl,
              tuneGrid = xgb_grid_linear,
              nthread = 1)
    }
  })
}

save(xgb_fit, file = "./cache/boosting_with_regularization_models.RData")
```

## L2-regularization results

We see lowest RMSE's when `lambda` is set to 100. It looks as though the L2-regularization produces significantly better results than L1. This is most likely due to the strong correlation in the predictor variables. L1 regularization will give quite different values to correlated variables, whereas L2 regularization gives similar values (see ISLR p. 261).

```{r model_performance_l2}
# xgb_l2_fit
# 
# ggplot(xgb_l2_fit) +
#   scale_x_log10() +
#   ggtitle("L2 model performance")
```

## CV performance

```{r cv_performance}
results_df <- NULL
for (iZ in names(xgb_fit)) {
  for (iM in names(xgb_fit[[iZ]])) {
    results_df <- data.frame(Zone = iZ,
                             Model = iM,
                             xgb_fit[[iZ]][[iM]]$results) %>% 
      bind_rows(results_df, .)
  }
}

results_df <- results_df %>% 
  mutate(Zone = factor(Zone, levels = all_zones, ordered = T)) # order subplots

ggplot() +
  geom_hline(data = filter(results_df, Model == "Manual"),
             aes(yintercept = RMSE, colour = Model),
             linetype = "dashed") +
  geom_line(data = filter(results_df, Model == "L1-regularization"), 
            aes(x = alpha, y = RMSE, colour = Model)) +
  geom_line(data = filter(results_df, Model == "L2-regularization"), 
            aes(x = lambda, y = RMSE, colour = Model)) +
  ggtitle("5-fold CV RMSE by zone") +
  xlab("Penalty") +
  scale_x_continuous(trans = "log",
                     labels = function(x) round(x, 0)) +
  facet_wrap(~Zone, scales = "free_y", nrow = 2)
```

# Test data results

RMSE scores on the test data set for the three model types are calculated below. Strangely, we are seeing better performance on the test data set, which may mean something is wrong. There is a chance there is less noisy data in 2016 which could also explain things, but this needs to be confirmed.

```{r test_performance}
test_results_df <- NULL
for (iZ in names(xgb_fit)) {
  for (iM in names(xgb_fit[[iZ]])) {
    test_data_zone <- filter(smd_test, Zone == iZ)
    
    test_results_df <- data.frame(
      Zone = iZ,
      Model = iM,
      Demand = test_data_zone$Demand,
      Demand_pred = predict(xgb_fit[[iZ]][[iM]], test_data_zone)) %>% 
      bind_rows(test_results_df, .)
  }
}

test_rmse <- test_results_df %>% 
  group_by(Zone, Model) %>% 
  summarise(rmse = mean((Demand_pred - Demand)^2)^0.5) %>% 
  ungroup()

zone_order <- test_rmse %>% 
  filter(Model == "Manual") %>% 
  arrange(rmse) %>% 
  .$Zone

test_rmse %>% 
  mutate(Zone = factor(Zone, levels = zone_order, ordered = TRUE)) %>% 
  ggplot(aes(x = Zone, y = rmse, colour = Model, group = Model)) +
  geom_point(size = 2) +
  #geom_line() +
  coord_flip() +
  ggtitle("RMSE for models on test data set")
```


# Predicted time-series plots

**TODO**

Do we see a lot of variance?
