---
title: "Optimal weighted hierarchical reconciliation"
author: "Cameron Roach"
date: "23 January 2017"
output:
  pdf_document:
    includes:
      in_header: mystyles.sty
    toc: yes
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
---


```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	dev = "png",
	dpi = 150
)
rm(list=ls())

require(dplyr)
require(tidyr)
require(readxl)
require(lubridate)
require(ggplot2)
theme_set(theme_bw())
require(plotly)
require(caret)
require(myhelpr)
require(doMC)
require(gefcom2017)
require(WriteXLS)
require(ModelMetrics)
require(knitr)

registerDoMC(cores = 11)

# Inputs
load_zones_ma <- c("SEMASS", "WCMASS", "NEMASSBOST")
load_zones <- c("ME", "NH", "VT", "CT", "RI", load_zones_ma)
agg_zones <- c("TOTAL", "MASS")
all_zones <- c("TOTAL", "ME", "NH", "VT", "CT", "RI", "MASS", load_zones_ma) # hierarchical order
hts_recon_flag <- TRUE
n_sims <- 100
fcst_start_date <- dmy("1/2/2016")
fcst_end_date <- dmy("1/3/2016") - 1

# Load data
smd <- load_smd_data(load_zones, root_dir = ".")
smd <- clean_smd_data(smd, root_dir = ".")

# separate data frames for aggregated zones because may change modelling, 
# e.g., remove average of variables and include all individual ones.
smd_mass <- smd %>% 
  filter(Zone %in% load_zones_ma) %>% 
  group_by(Date, Hour, Holiday, Holiday_flag, ts, Period, Year, Month, DoW,
           DoY, Weekend) %>% 
  summarise(Demand = sum(Demand),
            DryBulb = mean(DryBulb),
            DewPnt = mean(DewPnt),
            DryDewDiff = mean(DryDewDiff)) %>% 
  ungroup() %>% 
  mutate(Zone = "MASS")

smd_total <- smd %>% 
  group_by(Date, Hour, Holiday, Holiday_flag, ts, Period, Year, Month, DoW,
           DoY, Weekend) %>% 
  summarise(Demand = sum(Demand),
            DryBulb = mean(DryBulb),
            DewPnt = mean(DewPnt),
            DryDewDiff = mean(DryDewDiff)) %>% 
  ungroup() %>% 
  mutate(Zone = "TOTAL")

# create lagged predictors
smd <- smd %>%
  group_by(Zone) %>% 
  do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = 1:72)) %>% 
  ungroup() %>% 
  filter(Year >= 2009)

smd_mass <- smd_mass %>% 
  do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = 1:72)) %>% 
  filter(Year >= 2009)

smd_total <- smd_total %>% 
  do(get_lagged_vars(., c("DryBulb", "DewPnt"), lags = 1:72)) %>% 
  filter(Year >= 2009)
```

# Introduction

Our objective is to reconcile hierarchical forecasts for the GEFCOM 2017 competition. The hierarchy is unbalanced with two levels. Forecasts have been produced using the L1-penalised linear boosting models. Forecasts have been created using the training data. We do not concern ourselves with fitting the models to an out-of-sample test set as we are only interested in how well the reconciliation methods work.

# GEFCom 2017 data

## Load models and fit

First, load models from `smd-load-forecasting.Rmd`.

```{r load_models}
load("./cache/smd_load_forecasting.RData")
```


## Hierarchical reconciliation

We are dealing with the hierarchy shown in figure \ref{fig:hierarchy}.


\begin{figure}[h]
\begin{tikzpicture}[sibling distance=6em,
  font=\sffamily\small,
  every node/.style = {shape=rectangle,
    draw, align=center}]
  \node {TOTAL}
    child { node {ME} }
    child { node {NH} }
    child { node {VT} }
    child { node {CT} }
    child { node {RI} }
    child { node {MASS} 
      child { node {SEMASS} }
      child { node {WCMASS} }
      child { node {NEMASSBOST} } };
\end{tikzpicture}
\caption{Load forecasting hierarchy for GEFCOM 2017}
\label{fig:hierarchy}
\end{figure}


Figure \ref{fig:hierarchy} can be represented in matrix notation using the summing matrix $S$,

$$
\begin{bmatrix}
  y_{TOTAL,t} \\
  y_{ME,t} \\
  y_{NH,t} \\
  y_{VT,t} \\
  y_{CT,t} \\
  y_{RI,t} \\
  y_{MASS,t} \\
  y_{SEMASS,t} \\
  y_{WCMASS,t} \\
  y_{NEMASSBOST,t} \\
\end{bmatrix} =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
    0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
  \end{bmatrix}
  \begin{bmatrix}
    y_{ME,t} \\
    y_{NH,t} \\
    y_{VT,t} \\
    y_{CT,t} \\
    y_{RI,t} \\
    y_{SEMASS,t} \\
    y_{WCMASS,t} \\
    y_{NEMASSBOST,t} \\
  \end{bmatrix}
$$

$$
\mathbf{y}_t = \mathbf{S}\mathbf{y}_{8,t}
$$

Adjusted forecasts $\tilde{\mathbf{y}}_t$ can be calculated using optimal forecast reconciliation,

$$
\tilde{\mathbf{y}}_t = \mathbf{S}(\mathbf{S'S})^{-1}\mathbf{S'}\hat{\mathbf{y}}_t
$$

where $\hat{\mathbf{y}}_t$ are the original forecasts for each zone at time $t$.

We propose a modified version of the optimal forecast reconciliation method that incorporates weights of the zones based on the average demand in each node and aggregated node,

$$
\tilde{\mathbf{y}}_t = \mathbf{S}(\mathbf{S'WS})^{-1}\mathbf{S'W}\hat{\mathbf{y}}_t,
$$

where the weights are given as,

$$
\mathbf{W} = \sum_{n} \bar{y}_n \cdot
\begin{bmatrix}
  \frac{1}{\bar{y}_{TOTAL}} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
  0 & \frac{1}{\bar{y}_{ME}} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
  0 & 0 & \frac{1}{\bar{y}_{NH}} & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & \frac{1}{\bar{y}_{VT}} & 0 & 0 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & \frac{1}{\bar{y}_{CT}} & 0 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 0 & \frac{1}{\bar{y}_{RI}} & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 0 & 0 & \frac{1}{\bar{y}_{MASS}} & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{1}{\bar{y}_{SEMASS}} & 0 & 0\\
  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{1}{\bar{y}_{WCMASS}} & 0\\
  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \frac{1}{\bar{y}_{NEMASSBOST}}\\
\end{bmatrix},
$$

and $\bar{y}_n$ is the average demand for node $n$.

## Fit values

Demand predictions are now calculated for every zone over the entire duration of the training set. This gives 8 child node time-series and 2 aggregated nodes. Each time-series starts in 2009 and ends 2016.

```{r predict_values}
fcst_df_vars <- c("ts", "Date", "Hour", "DoY", "DoW", "Year", "Month", 
                  "Period", "Zone", "Holiday", "Holiday_flag", "Weekend",
                  "Demand", "Prediction")
fcst_df <- NULL
for (iZ in load_zones) {
  # remember to scale predictor data again!
  fcst_df <- smd %>% 
    filter(Zone == iZ) %>% 
    data.frame(.,
               Prediction = predict(xgb_fit[[iZ]], .)) %>% 
    select(one_of(fcst_df_vars)) %>% 
    bind_rows(fcst_df)
}

# MASS predictions
fcst_df <- smd_mass %>% 
  data.frame(.,
             Prediction = predict(xgb_fit[["MASS"]], .)) %>% 
  select(one_of(fcst_df_vars)) %>% 
  bind_rows(fcst_df)

# TOTAL predictions
fcst_df <- smd_total %>% 
  data.frame(.,
             Prediction = predict(xgb_fit[["TOTAL"]], .)) %>% 
  select(one_of(fcst_df_vars)) %>% 
  bind_rows(fcst_df)
```

_Note: This is now different to what happens in the simulation. In the simulation we also add on residuals and THEN reconcile. Here we are only doing predictions and then reconciling. Keep that in mind when discussing because they are two very different things and both deserve attention._

## Reconciliation

Now that we have generated a hierarchical time-series we can attempt to reconcile the two zones. Reconciliation is done using the optimal reconciliation and weighted optimal reconciliation methods.

```{r hierarchical_reconciliation}
S <- matrix(ncol = 8, nrow = 10, byrow = TRUE,
            c(1,1,1,1,1,1,1,1,
              1,0,0,0,0,0,0,0,
              0,1,0,0,0,0,0,0,
              0,0,1,0,0,0,0,0,
              0,0,0,1,0,0,0,0,
              0,0,0,0,1,0,0,0,
              0,0,0,0,0,1,1,1,
              0,0,0,0,0,1,0,0,
              0,0,0,0,0,0,1,0,
              0,0,0,0,0,0,0,1))

y_mean <- c(mean(smd_total$Demand),
            mean(smd[smd$Zone == "ME",]$Demand),
            mean(smd[smd$Zone == "NH",]$Demand),
            mean(smd[smd$Zone == "VT",]$Demand),
            mean(smd[smd$Zone == "CT",]$Demand),
            mean(smd[smd$Zone == "RI",]$Demand),
            mean(smd_mass$Demand),
            mean(smd[smd$Zone == "SEMASS",]$Demand),
            mean(smd[smd$Zone == "WCMASS",]$Demand),
            mean(smd[smd$Zone == "NEMASSBOST",]$Demand))
W <- diag(sum(y_mean) / y_mean)

h_rec <- function(x, S) {
  S %*% solve(t(S) %*% S) %*% t(S) %*% x
}

h_rec_w <- function(x, S, W) {
  S %*% solve(t(S) %*% W %*% S) %*% t(S) %*% W %*% x
}

hts_raw <- fcst_df %>% 
  select(Zone, ts, Date, Hour, Prediction) %>% 
  spread(Zone, Prediction) %>% 
  select(ts, Date, Hour, TOTAL, ME, NH, VT, CT, RI, MASS,
         SEMASS, WCMASS, NEMASSBOST)

hts_rec <- t(apply(as.matrix(hts_raw[,-c(1:3)]), 1, h_rec, S))
hts_rec_w <- t(apply(as.matrix(hts_raw[,-c(1:3)]), 1, h_rec_w, S, W))

colnames(hts_rec) <- c("TOTAL", "ME", "NH", "VT", "CT", "RI", "MASS",
                       "SEMASS", "WCMASS", "NEMASSBOST")
colnames(hts_rec_w) <- c("TOTAL", "ME", "NH", "VT", "CT", "RI", "MASS",
                         "SEMASS", "WCMASS", "NEMASSBOST")

# convert back to a dataframe and correct structure
hts_rec <- bind_cols(hts_raw[,c(1:3)], as.data.frame(hts_rec)) %>% 
  gather(Zone, Prediction_rec, -c(ts, Date, Hour)) %>% 
  rename(Prediction_h = Prediction_rec)
hts_rec_w <- bind_cols(hts_raw[,c(1:3)], as.data.frame(hts_rec_w)) %>% 
  gather(Zone, Prediction_rec, -c(ts, Date, Hour)) %>% 
  rename(Prediction_wh = Prediction_rec)

# Add everything back into data frame
fcst_df <- fcst_df %>% 
  inner_join(hts_rec) %>% 
  inner_join(hts_rec_w)
```

## Results

The weighted optimal reconciliation method appears to outperform the unreconciled and optimal reconciliation approaches based on RMSE scores.

```{r reconciliation_plot, fig.height=4}
p_date <- unique(fcst_df$Date)[2500 + 8:12]

for (iZ in all_zones) {
  p <- ggplot() +
    geom_line(data = fcst_df %>% 
                filter(Zone == iZ,
                       Date %in% p_date) %>% 
                mutate(Zone = factor(Zone, levels = all_zones, ordered = TRUE)),
              aes(x = ts, y = Prediction, colour = "Unreconciled")) +
    geom_line(data = fcst_df %>% 
                filter(Zone == iZ,
                       Date %in% p_date) %>% 
                mutate(Zone = factor(Zone, levels = all_zones, ordered = TRUE)),
              aes(x = ts, y = Prediction_h, colour = "Reconciled")) +
    geom_line(data = fcst_df %>% 
                filter(Zone == iZ,
                       Date %in% p_date) %>% 
                mutate(Zone = factor(Zone, levels = all_zones, ordered = TRUE)),
              aes(x = ts, y = Prediction_wh, colour = "Reconciled (weighted)")) +
    geom_point(data = fcst_df %>% 
                 filter(Zone == iZ,
                        Date %in% p_date) %>% 
                 mutate(Zone = factor(Zone, levels = all_zones, ordered = TRUE)),
               aes(x = ts, y = Demand, colour = "Actual"),
               shape = 21,
               size = 1,
               alpha = 0.5) +
    ylab("Demand") +
    scale_colour_manual(name = NULL,
                        values = c("Reconciled (weighted)" = "black",
                                   "Reconciled" = "red",
                                   "Unreconciled" = "darkgrey",
                                   "Actual" = "darkgreen")) +
    ggtitle("Hierarchical forecast reconcilation",
            paste("Optimal combination approach, Zone:", iZ))
  
  print(p)
}
```


```{r rmse_calculations}
rmse_df <- fcst_df %>% 
  group_by(Zone) %>% 
  do(data.frame(Unreconciled = rmse(.$Demand, .$Prediction),
                Reconciled = rmse(.$Demand, .$Prediction_h),
                Reconciled_weighted = rmse(.$Demand, .$Prediction_wh)))

kable(rmse_df,
      caption = "RMSE scores split by zone",
      digits = 2)

rmse_df %>%
  ungroup() %>% 
  mutate(Zone = factor(Zone, levels = all_zones, ordered = TRUE)) %>% 
  gather(Method, RMSE, -Zone) %>% 
  mutate(Method = factor(Method, levels = c("Unreconciled", "Reconciled", "Reconciled_weighted")), ordered = TRUE) %>% 
  ggplot(aes(x = Zone, y = RMSE, fill = Method)) + 
  geom_col(position = "dodge") +
  #geom_point() +
  ggtitle("RMSE for reconcilation methods",
          "For each node") +
  theme(axis.text.x=element_text(angle=45, hjust = 1))

rmse_df <- fcst_df %>% 
  do(data.frame(Unreconciled = rmse(.$Demand, .$Prediction),
                Reconciled = rmse(.$Demand, .$Prediction_h),
                Reconciled_weighted = rmse(.$Demand, .$Prediction_wh)))

kable(rmse_df,
      caption = "RMSE scores averaged across all zones",
      digits = 2)

rmse_df %>%
  gather(Method, RMSE) %>% 
  mutate(Method = factor(Method, levels = c("Unreconciled", "Reconciled", "Reconciled_weighted")), ordered = TRUE) %>% 
  ggplot(aes(x = Method, y = RMSE, fill = Method)) + 
  geom_col(position = "dodge") +
  #geom_point() +
  ggtitle("RMSE for reconcilation methods",
          "Averaged over all child and parent nodes") +
  theme(axis.text.x=element_text(angle=45, hjust = 1),
        legend.position = "none")
```

