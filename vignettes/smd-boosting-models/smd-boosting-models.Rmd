---
title: "SMD Boosting Models"
author: Cameron Roach
output: 
  html_notebook: 
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
rm(list=ls())

require(dplyr)
require(tidyr)
require(readxl)
require(lubridate)
require(ggplot2)
require(plotly)
require(DT)
require(caret)
require(myhelpr)

source("../../R/loadData.R")

load_zones_ma <- c("SEMASS", "WCMASS", "NEMASSBOST")
load_zones <- c("ME", "NH", "VT", "CT", "RI", load_zones_ma)

smd <- load_smd_data(root_dir = "./../..", load_zones)
```

# TODO

* Think about converting Hour from hour ending to hour beginning. Ensures that the date and hour match up.
* It looks like there is one observation missing early March and two observations being grouped together at beginning of November for each year and zone. Most likely daylight savings. Check if this is in the source data or if it is caused by aggregation in the load_smd_data function.
* Still need to figure out why those NA values are sneaking in to smd dataframe.
* Maybe start a new vignette investigating the impact of lags on xgboost accuracy? Use a cut down data set size (1 year, demand and temperature only?) to speed up model fitting. From some basic testing it looks as though lags do improve the fit.
    * Remove the adhoc lagged variable creation from within this doc (model 3)
    * Decide if I also want to include lagged demand values.
    * Decide if I should use xgbTree or xgbLinear

# Introduction

This notebook explores fitting various models to produce quantile forecasts for the 2017 GEFCom-D competition.

Models will be fit using the `xgboost` package. Performance is assessed using time-series cross validation. The performance score is based on the pinball loss function.

## Data

Since this is for the GEFCom-D track of the competition only given data may be used. Hence, we will be able to use data for

* dates
* hour ending,
* holidays,
* dry bulb temperature,
* dew point temperature,
* demand.

Additionally, several dummy variables have been calculated. These include,

* month of year,
* period of day (1-24, ordered factor),
* season,
* holiday flag (boolean),
* day of week,
* weekend (boolean),
* dry bulb and dew point temperature differences.

Period of day should be used in place of Hour in modelling as it is an ordered factor.

## Output

We wish to produce quantile forecasts (10, 20, ..., 90) for each day in a future month. These forecasts will be produced for each of the zones and also the two aggregated zones (Massachusetts and total of all).

Since we are not provided forecast weather data for the month we will need to rely on a bootstrapping approach to create a distribution.

# Modelling

This project will be broken into several parts.

1. Fit a model using 2011-2016 data. This data contains public holiday information. Test several models using time-series cross-validation.
2. Bootstrap weather data (all data 2003-2016). Check for any trends - unlikely over that period. Feed bootstrapped data into model to produce demand traces.
3. Reconcile aggregated and zonal demand forecasts (for each bootstrap sample) using `hts` package.
4. Bootstrap residuals and add to fitted demand values.
5. Calculate quantiles for each day in month of interest based on bootstrapped results.


Note that since we are not given forecasts of weather data we will have to rely on a bootstrapping approach to determine quantiles, rather than a quantile regression type of model.

## Model training

```{r}
iZ <- "SEMASS"

train_data <- filter(smd,
                     Year >= 2011,
                     Year < 2015,
                     Zone == iZ) %>% 
  mutate(DryBulb_lag1 = lag(DryBulb, 1))

test_data <- filter(smd,
                    Year >= 2015,
                    Zone == iZ) %>% 
  mutate(DryBulb_lag1 = lag(DryBulb, 1))

maeSummary <- function (data,
                        lev = NULL,
                        model = NULL) {
  out <- mean(abs(data$obs - data$pred), na.rm=TRUE)
  names(out) <- "MAE"
  out
}

fitControl <- trainControl(
  method = "timeslice",
  initialWindow = ceiling(dim(train_data)[1]*0.7),
  horizon=24*7,
  fixedWindow=FALSE,
  summaryFunction = maeSummary)

smd_fit <- list()
#mae <- rep(NA, 24)

system.time({
  smd_fit[[1]] <- train(Demand ~ poly(DryBulb, 2, raw = TRUE) + Weekend + DoW + Period + Month,
                        data = train_data,
                        method="lm",
                        metric="MAE",
                        maximize = FALSE)#,
  #trControl = fitControl)
})


xgb_grid <- expand.grid(nrounds = 500,
                          eta = c(0.01,0.1),
                          max_depth = c(2,6,10),
                          gamma = 1,
                          colsample_bytree = 1,
                          min_child_weight = 1)

system.time({
  smd_fit[[2]] <- train(Demand ~ DryBulb + DoW + Holiday_flag + Period + DoY,
                        data = train_data,
                        method="xgbTree",
                        metric="MAE",
                        maximize = FALSE,
                        tuneGrid = xgb_grid)#,
  #trControl = fitControl)
  #mae[i+1] <- model_h[[i+1]]$results$MAE
})

system.time({
  smd_fit[[3]] <- train(Demand ~ DryBulb + DryBulb_lag1 + DoW + Holiday_flag + Period + DoY,
                        data = train_data,
                        method="xgbTree",
                        metric="MAE",
                        maximize = FALSE,
                        tuneGrid = xgb_grid)#,
  #trControl = fitControl)
  #mae[i+1] <- model_h[[i+1]]$results$MAE
})
```

```{r}
# train_data$Demand_pred1 <- predict(smd_fit[[1]], newdata = train_data)
# train_data$Demand_pred2 <- predict(smd_fit[[2]], newdata = train_data)
test_data$Demand_pred1 <- predict(smd_fit[[1]], newdata = test_data)
test_data$Demand_pred2 <- predict(smd_fit[[2]], newdata = test_data)
test_data$Demand_pred3 <- predict(smd_fit[[3]], newdata = test_data)

# train_data %>% 
#   filter(Year == 2014) %>% 
test_data %>%
  filter(Year == 2015, Month %in% c("Jun")) %>%
  #filter(Year == 2015) %>% 
  plot_ly(x = ~ts) %>% 
  add_trace(y = ~Demand, name = "Actual", type = "scatter", mode = "lines") %>% 
  add_trace(y = ~Demand_pred1, name = "lm", type = "scatter", mode = "lines") %>% 
  add_trace(y = ~Demand_pred2, name = "xgbTree", type = "scatter", mode = "lines") %>% 
  add_trace(y = ~Demand_pred3, name = "xgbTree_lag", type = "scatter", mode = "lines") %>% 
  layout(title = "Actuals and model predictions")
```


### Performance on test set

Overall performance

```{r}
test_data %>% 
  mutate(se1 = (Demand - Demand_pred1)^2,
         se2 = (Demand - Demand_pred2)^2,
         se3 = (Demand - Demand_pred3)^2) %>% 
  summarise(mse1 = mean(se1)^0.5,
            mse2 = mean(se2)^0.5,
            mse3 = mean(se3)^0.5) %>% 
  gather(Model, Score) %>% 
  datatable()
```

Performance by load zone

```{r}
test_data %>% 
  mutate(se1 = (Demand - Demand_pred1)^2,
         se2 = (Demand - Demand_pred2)^2) %>% 
  group_by(Zone) %>% 
  summarise(mse1 = mean(se1)^0.5,
            mse2 = mean(se2)^0.5) %>% 
  gather(Model, Score, -Zone) %>% 
  datatable()
```


## Bootstrapping

During the bootstrapping stage care must be taken to preserve correlations present in the historical data. This includes temporal correlations such as,

* daily weather correlations,

and spatial correlations such as,

* demand correlation between zones.

### Weather bootstrapping

Historical weather data will be bootstrapped in blocks that preserve intra-day weather correlation. _This is similar to the approach by Rob and Shu._

### Residual bootstrapping

In addition to bootstrapping historical weather data to obtain weather scenarios, bootstrapping of the residuals needs to be undertaken. These bootstrapped residuals can then be added back to the fitted values to produce the estimated demand values.

We expect to see correlation in demand between zones. To preserve this correlation, a blocked bootstrap approach will be taken, where each block contains all the zones. This way, zones are shifted together thereby preserving there correlations. This has the advantage of allowing us to simply sum all of the residuals within an aggregated zone to obtain its aggregated residual. _See Souhaib "bagging quantile regression"._

## Aggregated forecast reconciliation

R's `hts` package can be used to reconcile the means of heirarchical forecasts. Since we are forecasting the conditional mean on the bootstrap samples we can use `hts` to reconcile the aggregated means and zone means. Empirical quantiles may then be calculated from these reconciled forecasts for each zone and aggregated zone.

## Quantile calculations

Quantiles may be calculated for each hour of each day by simply calculating the empirical quantiles of the bootstrapped demand traces. R's `quantile` function is all that is needed for this.

# Performance

Performance will be assessed using the pinball loss function.