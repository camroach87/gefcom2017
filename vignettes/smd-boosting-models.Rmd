---
title: "SMD Boosting Models"
author: Cameron Roach
output: 
  html_notebook: 
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

rm(list=ls())

require(dplyr)
require(tidyr)
require(readxl)
require(lubridate)
require(ggplot2)
require(plotly)
require(DT)
require(caret)
require(myhelpr)
require(doMC)

registerDoMC(cores = 4)

source("../R/loadData.R")
source("../R/clean-smd-data.R")

load_zones_ma <- c("SEMASS", "WCMASS", "NEMASSBOST")
load_zones <- c("ME", "NH", "VT", "CT", "RI", load_zones_ma)

smd <- load_smd_data(load_zones, root_dir = "./..")
smd <- clean_smd_data(smd, root_dir = "./..")

iZ <- "ME"

smd <- smd %>%
  filter(Zone == iZ) %>% 
  mutate(DryBulb_lag1 = lag(DryBulb, 1),
         DryBulb_lag2 = lag(DryBulb, 2),
         DryBulb_lag3 = lag(DryBulb, 3),
         DryBulb_lag4 = lag(DryBulb, 4),
         DryBulb_lag5 = lag(DryBulb, 5),
         DryBulb_lag6 = lag(DryBulb, 6),
         DryBulb_lag7 = lag(DryBulb, 7),
         DryBulb_lag8 = lag(DryBulb, 8),
         DryBulb_lag9 = lag(DryBulb, 9),
         DryBulb_lag10 = lag(DryBulb, 10),
         DryBulb_lag11 = lag(DryBulb, 11),
         DryBulb_lag12 = lag(DryBulb, 12),
         DryBulb_lag13 = lag(DryBulb, 13),
         DryBulb_lag14 = lag(DryBulb, 14),
         DryBulb_lag15 = lag(DryBulb, 15),
         DryBulb_lag16 = lag(DryBulb, 16),
         DryBulb_lag17 = lag(DryBulb, 17),
         DryBulb_lag18 = lag(DryBulb, 18),
         DryBulb_lag19 = lag(DryBulb, 19),
         DryBulb_lag20 = lag(DryBulb, 20),
         DryBulb_lag21 = lag(DryBulb, 21),
         DryBulb_lag22 = lag(DryBulb, 22),
         DryBulb_lag23 = lag(DryBulb, 23),
         DryBulb_lag24 = lag(DryBulb, 24),
         DryBulb_lag25 = lag(DryBulb, 25),
         DryBulb_lag26 = lag(DryBulb, 26),
         DryBulb_lag27 = lag(DryBulb, 27),
         DryBulb_lag28 = lag(DryBulb, 28),
         DryBulb_lag29 = lag(DryBulb, 29),
         DryBulb_lag30 = lag(DryBulb, 30),
         DryBulb_lag31 = lag(DryBulb, 31),
         DryBulb_lag32 = lag(DryBulb, 32),
         DryBulb_lag33 = lag(DryBulb, 33),
         DryBulb_lag34 = lag(DryBulb, 34),
         DryBulb_lag35 = lag(DryBulb, 35),
         DryBulb_lag36 = lag(DryBulb, 36),
         DryBulb_lag37 = lag(DryBulb, 37),
         DryBulb_lag38 = lag(DryBulb, 38),
         DryBulb_lag39 = lag(DryBulb, 39),
         DryBulb_lag40 = lag(DryBulb, 40),
         DryBulb_lag41 = lag(DryBulb, 41),
         DryBulb_lag42 = lag(DryBulb, 42),
         DryBulb_lag43 = lag(DryBulb, 43),
         DryBulb_lag44 = lag(DryBulb, 44),
         DryBulb_lag45 = lag(DryBulb, 45),
         DryBulb_lag46 = lag(DryBulb, 46),
         DryBulb_lag47 = lag(DryBulb, 47),
         DryBulb_lag48 = lag(DryBulb, 48),
         DryBulb_lag49 = lag(DryBulb, 49),
         DryBulb_lag50 = lag(DryBulb, 50),
         DryBulb_lag51 = lag(DryBulb, 51),
         DryBulb_lag52 = lag(DryBulb, 52),
         DryBulb_lag53 = lag(DryBulb, 53),
         DryBulb_lag54 = lag(DryBulb, 54),
         DryBulb_lag55 = lag(DryBulb, 55),
         DryBulb_lag56 = lag(DryBulb, 56),
         DryBulb_lag57 = lag(DryBulb, 57),
         DryBulb_lag58 = lag(DryBulb, 58),
         DryBulb_lag59 = lag(DryBulb, 59),
         DryBulb_lag60 = lag(DryBulb, 60),
         DryBulb_lag61 = lag(DryBulb, 61),
         DryBulb_lag62 = lag(DryBulb, 62),
         DryBulb_lag63 = lag(DryBulb, 63),
         DryBulb_lag64 = lag(DryBulb, 64),
         DryBulb_lag65 = lag(DryBulb, 65),
         DryBulb_lag66 = lag(DryBulb, 66),
         DryBulb_lag67 = lag(DryBulb, 67),
         DryBulb_lag68 = lag(DryBulb, 68),
         DryBulb_lag69 = lag(DryBulb, 69),
         DryBulb_lag70 = lag(DryBulb, 70),
         DryBulb_lag71 = lag(DryBulb, 71),
         DryBulb_lag72 = lag(DryBulb, 72),
         DewPnt_lag1 = lag(DewPnt, 1),
         DewPnt_lag2 = lag(DewPnt, 2),
         DewPnt_lag3 = lag(DewPnt, 3),
         DewPnt_lag4 = lag(DewPnt, 4),
         DewPnt_lag5 = lag(DewPnt, 5),
         DewPnt_lag6 = lag(DewPnt, 6),
         DewPnt_lag7 = lag(DewPnt, 7),
         DewPnt_lag8 = lag(DewPnt, 8),
         DewPnt_lag9 = lag(DewPnt, 9),
         DewPnt_lag10 = lag(DewPnt, 10),
         DewPnt_lag11 = lag(DewPnt, 11),
         DewPnt_lag12 = lag(DewPnt, 12),
         DewPnt_lag13 = lag(DewPnt, 13),
         DewPnt_lag14 = lag(DewPnt, 14),
         DewPnt_lag15 = lag(DewPnt, 15),
         DewPnt_lag16 = lag(DewPnt, 16),
         DewPnt_lag17 = lag(DewPnt, 17),
         DewPnt_lag18 = lag(DewPnt, 18),
         DewPnt_lag19 = lag(DewPnt, 19),
         DewPnt_lag20 = lag(DewPnt, 20),
         DewPnt_lag21 = lag(DewPnt, 21),
         DewPnt_lag22 = lag(DewPnt, 22),
         DewPnt_lag23 = lag(DewPnt, 23),
         DewPnt_lag24 = lag(DewPnt, 24),
         DewPnt_lag25 = lag(DewPnt, 25),
         DewPnt_lag26 = lag(DewPnt, 26),
         DewPnt_lag27 = lag(DewPnt, 27),
         DewPnt_lag28 = lag(DewPnt, 28),
         DewPnt_lag29 = lag(DewPnt, 29),
         DewPnt_lag30 = lag(DewPnt, 30),
         DewPnt_lag31 = lag(DewPnt, 31),
         DewPnt_lag32 = lag(DewPnt, 32),
         DewPnt_lag33 = lag(DewPnt, 33),
         DewPnt_lag34 = lag(DewPnt, 34),
         DewPnt_lag35 = lag(DewPnt, 35),
         DewPnt_lag36 = lag(DewPnt, 36),
         DewPnt_lag37 = lag(DewPnt, 37),
         DewPnt_lag38 = lag(DewPnt, 38),
         DewPnt_lag39 = lag(DewPnt, 39),
         DewPnt_lag40 = lag(DewPnt, 40),
         DewPnt_lag41 = lag(DewPnt, 41),
         DewPnt_lag42 = lag(DewPnt, 42),
         DewPnt_lag43 = lag(DewPnt, 43),
         DewPnt_lag44 = lag(DewPnt, 44),
         DewPnt_lag45 = lag(DewPnt, 45),
         DewPnt_lag46 = lag(DewPnt, 46),
         DewPnt_lag47 = lag(DewPnt, 47),
         DewPnt_lag48 = lag(DewPnt, 48),
         DewPnt_lag49 = lag(DewPnt, 49),
         DewPnt_lag50 = lag(DewPnt, 50),
         DewPnt_lag51 = lag(DewPnt, 51),
         DewPnt_lag52 = lag(DewPnt, 52),
         DewPnt_lag53 = lag(DewPnt, 53),
         DewPnt_lag54 = lag(DewPnt, 54),
         DewPnt_lag55 = lag(DewPnt, 55),
         DewPnt_lag56 = lag(DewPnt, 56),
         DewPnt_lag57 = lag(DewPnt, 57),
         DewPnt_lag58 = lag(DewPnt, 58),
         DewPnt_lag59 = lag(DewPnt, 59),
         DewPnt_lag60 = lag(DewPnt, 60),
         DewPnt_lag61 = lag(DewPnt, 61),
         DewPnt_lag62 = lag(DewPnt, 62),
         DewPnt_lag63 = lag(DewPnt, 63),
         DewPnt_lag64 = lag(DewPnt, 64),
         DewPnt_lag65 = lag(DewPnt, 65),
         DewPnt_lag66 = lag(DewPnt, 66),
         DewPnt_lag67 = lag(DewPnt, 67),
         DewPnt_lag68 = lag(DewPnt, 68),
         DewPnt_lag69 = lag(DewPnt, 69),
         DewPnt_lag70 = lag(DewPnt, 70),
         DewPnt_lag71 = lag(DewPnt, 71),
         DewPnt_lag72 = lag(DewPnt, 72))

train_data <- filter(smd, Year >= 2012, Year < 2015)

test_data <- filter(smd, Year == 2015)
```

# TODO

* Think about converting Hour from hour ending to hour beginning. Ensures that the date and hour match up.
* Maybe have 2 models. One for DST months and another for non-daylight savings times?

# Introduction

This notebook explores fitting various models to produce quantile forecasts for the 2017 GEFCom-D competition.

Models will be fit using the `xgboost` package. Performance is assessed using time-series cross validation. The performance score is based on the pinball loss function.

## Data

Since this is for the GEFCom-D track of the competition only given data may be used. Hence, we will be able to use data for

* dates
* hour ending,
* holidays,
* dry bulb temperature,
* dew point temperature,
* demand.

Additionally, several dummy variables have been calculated. These include,

* month of year,
* period of day (1-24, ordered factor),
* season,
* holiday flag (boolean),
* day of week,
* weekend (boolean),
* dry bulb and dew point temperature differences.

Period of day should be used in place of Hour in modelling as it is an ordered factor.

## Output

We wish to produce quantile forecasts (10, 20, ..., 90) for each day in a future month. These forecasts will be produced for each of the zones and also the two aggregated zones (Massachusetts and total of all).

Since we are not provided forecast weather data for the month we will need to rely on a bootstrapping approach to create a distribution.

# Modelling

This project will be broken into several parts.

1. Fit a model using 2011-2016 data. This data contains public holiday information. Test several models using time-series cross-validation.
2. Bootstrap weather data (all data 2003-2016). Check for any trends - unlikely over that period. Feed bootstrapped data into model to produce demand traces.
3. Reconcile aggregated and zonal demand forecasts (for each bootstrap sample) using `hts` package.
4. Bootstrap residuals and add to fitted demand values.
5. Calculate quantiles for each day in month of interest based on bootstrapped results.


Note that since we are not given forecasts of weather data we will have to rely on a bootstrapping approach to determine quantiles, rather than a quantile regression type of model.

## Model training

We train a naive model (OLS), a linear boosted model without lags and the xgbLinear_lag5 model, which includes several lags. The xgbLinear_lag5 model differs to the one investigated in boosting-test.Rmd in that calendar effects have been added (DoW, Holiday_flag and DoY).

```{r}
xgb_fit <- list()

xgb_ctrl <- trainControl(method = "repeatedcv",
                         number = 5,
                         allowParallel = TRUE)

xgb_fit[["naive_linear"]] <- train(Demand ~ poly(DryBulb, 2) + Period + DoW + Holiday_flag + Month,
                                   data = train_data,
                                   method="lm",
                                   trControl = xgb_ctrl)

xgb_grid_linear <- expand.grid(nrounds = 300,
                               lambda = 0,
                               alpha = 0,
                               eta = c(0.01, 0.1))

system.time({
  xgb_fit[["xgbLinear"]] <-  train(Demand ~ DryBulb + Period + DoW + Holiday_flag + DoY,
                                   data = train_data,
                                   method="xgbLinear",
                                   trControl = xgb_ctrl,
                                   tuneGrid = xgb_grid_linear,
                                   nthread = 1) # this arg stops OpenMP automatically running on linux, which stuffs up when also using doMC
})

system.time({
  xgb_fit[["xgbLinear_lag5"]] <-  train(Demand ~ DryBulb + DryBulb_lag1 + DryBulb_lag2 + DryBulb_lag3 + DryBulb_lag4 + DryBulb_lag5 + DryBulb_lag6 + DryBulb_lag24 + DryBulb_lag48 + DryBulb_lag72 + Period + DoW + Holiday_flag + DoY,
                                   data = train_data,
                                   method="xgbLinear",
                                   trControl = xgb_ctrl,
                                   tuneGrid = xgb_grid_linear,
                                   nthread = 1) # this arg stops OpenMP automatically running on linux, which stuffs up when also using doMC
})

test_data$Demand_pred_lm <- predict(xgb_fit[["naive_linear"]], newdata = test_data)
test_data$Demand_pred_xgb <- predict(xgb_fit[["xgbLinear"]], newdata = test_data)
test_data$Demand_pred_xgb_lag5 <- predict(xgb_fit[["xgbLinear_lag5"]], newdata = test_data)

test_data %>%
  filter(Year == 2015, Month %in% c("Mar")) %>%
  plot_ly(x = ~ts) %>% 
  add_trace(y = ~Demand, name = "Actual", type = "scatter", mode = "lines") %>% 
  add_trace(y = ~Demand_pred_lm, name = "lm", type = "scatter", mode = "lines") %>% 
  add_trace(y = ~Demand_pred_xgb, name = "xgbLinear", type = "scatter",
            mode = "lines") %>% 
  add_trace(y = ~Demand_pred_xgb_lag5, name = "xgbLinear_lag5",
            type = "scatter", mode = "lines") %>% 
  layout(title = "Actuals and model predictions")
```

Worryingly, we can see that once daylight savings kicks in on March 8th, the time of the peak shifts back one period (from 19 to 20). So, basically, it looks like demand is behaving as before (peak still occurs at same non-DST time), but because we have shifted to DST it looks like the peak has been shifted back by an hour. This may be affecting the model fit. Should probably build two models


### Performance on test set

The results here will be slightly different to those in boost-tests.Rmd because we are including non-working days and calendar effects.

```{r}
test_data %>% 
  mutate(se_lm = (Demand - Demand_pred_lm)^2,
         se_xgb = (Demand - Demand_pred_xgb)^2,
         se_xgb_lag5 = (Demand - Demand_pred_xgb_lag5)^2) %>% 
  summarise(mse_lm = mean(se_lm)^0.5,
            mse_xgb = mean(se_xgb)^0.5,
            mse_xgb_lag5 = mean(se_xgb_lag5)^0.5) %>%
  gather(Model, Score) %>% 
  datatable()
```

Performance by load zone

```{r}
# test_data %>% 
#   mutate(se1 = (Demand - Demand_pred1)^2,
#          se2 = (Demand - Demand_pred2)^2) %>% 
#   group_by(Zone) %>% 
#   summarise(mse1 = mean(se1)^0.5,
#             mse2 = mean(se2)^0.5) %>% 
#   gather(Model, Score, -Zone) %>% 
#   datatable()
```


## Bootstrapping

During the bootstrapping stage care must be taken to preserve correlations present in the historical data. This includes temporal correlations such as,

* daily weather correlations,

and spatial correlations such as,

* demand correlation between zones.

### Weather bootstrapping

Historical weather data will be bootstrapped in blocks that preserve intra-day weather correlation. _This is similar to the approach by Rob and Shu._

### Residual bootstrapping

In addition to bootstrapping historical weather data to obtain weather scenarios, bootstrapping of the residuals needs to be undertaken. These bootstrapped residuals can then be added back to the fitted values to produce the estimated demand values.

We expect to see correlation in demand between zones. To preserve this correlation, a blocked bootstrap approach will be taken, where each block contains all the zones. This way, zones are shifted together thereby preserving there correlations. This has the advantage of allowing us to simply sum all of the residuals within an aggregated zone to obtain its aggregated residual. _See Souhaib "bagging quantile regression"._

## Aggregated forecast reconciliation

R's `hts` package can be used to reconcile the means of heirarchical forecasts. Since we are forecasting the conditional mean on the bootstrap samples we can use `hts` to reconcile the aggregated means and zone means. Empirical quantiles may then be calculated from these reconciled forecasts for each zone and aggregated zone.

## Quantile calculations

Quantiles may be calculated for each hour of each day by simply calculating the empirical quantiles of the bootstrapped demand traces. R's `quantile` function is all that is needed for this.

# Performance

Performance will be assessed using the pinball loss function.